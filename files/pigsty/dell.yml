---
#==============================================================#
# File      :   dell.yml
# Desc      :   Pigsty config for a Dell R730 KVM host
# Ctime     :   2023-07-20
# Mtime     :   2023-07-20
# Docs      :   https://vonng.github.io/pigsty/#/CONFIG
# Author    :   Ruohang Feng (rh@vonng.com)
# License   :   AGPLv3
#==============================================================#

#==============================================================#
# 10.10.10.1  <------- pg-dell-1  the bare metal 72C 256G 4T SSD
# Vagrantfile: vagrant/spec/dell.rb
#==============================================================#
all:

  children:

    nodes:
      hosts:
        10.10.10.11: { nodename: infra-1       ,cpu: 8 ,mem: 32768 ,image: generic/rocky9  }
        10.10.10.12: { nodename: infra-2       ,cpu: 8 ,mem: 32768 ,image: generic/rocky8  }
        10.10.10.13: { nodename: infra-3       ,cpu: 8 ,mem: 32768 ,image: generic/centos7 }
        10.10.10.14: { nodename: proxy-1       ,cpu: 2 ,mem: 4096  ,image: generic/rocky9  }
        10.10.10.15: { nodename: proxy-2       ,cpu: 2 ,mem: 4096  ,image: generic/rocky9  }
        10.10.10.17: { nodename: minio-1       ,cpu: 2 ,mem: 4096  ,image: generic/rocky9  }
        10.10.10.18: { nodename: minio-2       ,cpu: 2 ,mem: 4096  ,image: generic/rocky9  }
        10.10.10.19: { nodename: minio-3       ,cpu: 2 ,mem: 4096  ,image: generic/rocky9  }
        10.10.10.31: { nodename: etcd-1        ,cpu: 2 ,mem: 4096  ,image: generic/rocky9  }
        10.10.10.32: { nodename: etcd-2        ,cpu: 2 ,mem: 4096  ,image: generic/rocky9  }
        10.10.10.33: { nodename: etcd-3        ,cpu: 2 ,mem: 4096  ,image: generic/rocky9  }
        10.10.10.34: { nodename: etcd-4        ,cpu: 2 ,mem: 4096  ,image: generic/rocky9  }
        10.10.10.35: { nodename: etcd-5        ,cpu: 2 ,mem: 4096  ,image: generic/rocky9  }
        10.10.10.41: { nodename: pg-test-1     ,cpu: 2 ,mem: 4096  ,image: generic/rocky9  }
        10.10.10.42: { nodename: pg-test-2     ,cpu: 2 ,mem: 4096  ,image: generic/rocky9  }
        10.10.10.43: { nodename: pg-test-3     ,cpu: 2 ,mem: 4096  ,image: generic/rocky9  }
        10.10.10.44: { nodename: pg-test-4     ,cpu: 2 ,mem: 4096  ,image: generic/rocky9  }
        10.10.10.45: { nodename: pg-src-1      ,cpu: 2 ,mem: 4096  ,image: generic/rocky9  }
        10.10.10.46: { nodename: pg-src-2      ,cpu: 2 ,mem: 4096  ,image: generic/rocky9  }
        10.10.10.47: { nodename: pg-src-3      ,cpu: 2 ,mem: 4096  ,image: generic/rocky9  }
        10.10.10.48: { nodename: pg-dst-1      ,cpu: 8 ,mem: 8192  ,image: generic/rocky9  }
        10.10.10.49: { nodename: pg-dst-2      ,cpu: 1 ,mem: 1024  ,image: generic/rocky9  }
        10.10.10.50: { nodename: pg-citus0-1   ,cpu: 2 ,mem: 2048  ,image: generic/rocky9  }
        10.10.10.51: { nodename: pg-citus0-2   ,cpu: 2 ,mem: 2048  ,image: generic/rocky9  }
        10.10.10.52: { nodename: pg-citus1-1   ,cpu: 2 ,mem: 2048  ,image: generic/rocky9  }
        10.10.10.53: { nodename: pg-citus1-2   ,cpu: 2 ,mem: 2048  ,image: generic/rocky9  }
        10.10.10.54: { nodename: pg-citus2-1   ,cpu: 2 ,mem: 2048  ,image: generic/rocky9  }
        10.10.10.55: { nodename: pg-citus2-2   ,cpu: 2 ,mem: 2048  ,image: generic/rocky9  }
        10.10.10.56: { nodename: pg-citus3-1   ,cpu: 2 ,mem: 2048  ,image: generic/rocky9  }
        10.10.10.57: { nodename: pg-citus3-2   ,cpu: 2 ,mem: 2048  ,image: generic/rocky9  }
        10.10.10.58: { nodename: pg-citus4-1   ,cpu: 2 ,mem: 2048  ,image: generic/rocky9  }
        10.10.10.59: { nodename: pg-citus4-2   ,cpu: 2 ,mem: 2048  ,image: generic/rocky9  }
        10.10.10.71: { nodename: redis-test-1  ,cpu: 2 ,mem: 4096  ,image: generic/rocky9  }
        10.10.10.72: { nodename: redis-test-2  ,cpu: 2 ,mem: 4096  ,image: generic/rocky9  }
        10.10.10.73: { nodename: redis-test-3  ,cpu: 2 ,mem: 4096  ,image: generic/rocky9  }
        10.10.10.74: { nodename: redis-test-4  ,cpu: 2 ,mem: 4096  ,image: generic/rocky9  }
        10.10.10.75: { nodename: redis-test-5  ,cpu: 2 ,mem: 4096  ,image: generic/rocky9  }
        10.10.10.76: { nodename: redis-test-6  ,cpu: 2 ,mem: 4096  ,image: generic/rocky9  }
        10.10.10.82: { nodename: pg-v12-1      ,cpu: 1 ,mem: 2048  ,image: generic/rocky9  }
        10.10.10.83: { nodename: pg-v13-1      ,cpu: 1 ,mem: 2048  ,image: generic/rocky9  }
        10.10.10.84: { nodename: pg-v14-1      ,cpu: 1 ,mem: 2048  ,image: generic/rocky9  }
        10.10.10.85: { nodename: pg-v15-1      ,cpu: 1 ,mem: 2048  ,image: generic/rocky9  }
        10.10.10.86: { nodename: pg-v16-1      ,cpu: 1 ,mem: 2048  ,image: generic/rocky9  }



    #==========================================================#
    # infra: 3 nodes (el7,el8,el9) for observability
    #==========================================================#
    # scp ~/pigsty/dist/v2.2.0/pigsty-pkg-v2.2.0.el7.x86_64.tgz infra-1:/tmp/pkg.tgz; ssh infra-1 'sudo mkdir -p /www; sudo tar -xf /tmp/pkg.tgz -C /www'
    # scp ~/pigsty/dist/v2.2.0/pigsty-pkg-v2.2.0.el8.x86_64.tgz infra-2:/tmp/pkg.tgz; ssh infra-2 'sudo mkdir -p /www; sudo tar -xf /tmp/pkg.tgz -C /www'
    # scp ~/pigsty/dist/v2.2.0/pigsty-pkg-v2.2.0.el9.x86_64.tgz infra-3:/tmp/pkg.tgz; ssh infra-3 'sudo mkdir -p /www; sudo tar -xf /tmp/pkg.tgz -C /www'
    # ./infra.yml -l infra
    infra:  # the el7, el8, el9 infra nodes
      hosts:
        10.10.10.11: { infra_seq: 1 ,nodename: infra-1 ,repo_endpoint: 'http://10.10.10.11:80' ,node_repo_local_urls: ['http://127.0.0.1/pigsty.repo'] }
        10.10.10.12: { infra_seq: 2 ,nodename: infra-2 ,repo_endpoint: 'http://10.10.10.12:80' ,node_repo_local_urls: ['http://127.0.0.1/pigsty.repo'] }
        10.10.10.13: { infra_seq: 3 ,nodename: infra-3 ,repo_endpoint: 'http://10.10.10.13:80' ,node_repo_local_urls: ['http://127.0.0.1/pigsty.repo'] }
      vars:
        node_id_from_pg: false
        node_cluster: infra
        node_conf: oltp
        docker_enabled: true


    #==========================================================#
    # proxy: 2 nodes used as dedicated haproxy server
    #==========================================================#
    # ./node.yml -l proxy
    proxy:
      hosts:
        10.10.10.14: { nodename: proxy-1 }
        10.10.10.15: { nodename: proxy-2 }
      vars:
        node_cluster: proxy


    #==========================================================#
    # minio: 3 nodes used as dedicated minio cluster
    #==========================================================#
    # ./node.yml -l minio;  ./minio.yml -l minio;
    minio:   # access service via sss.pigsty:9002
      hosts:
        10.10.10.17: { minio_seq: 1 , nodename: minio-1 }
        10.10.10.18: { minio_seq: 2 , nodename: minio-2 }
        10.10.10.19: { minio_seq: 3 , nodename: minio-3 }
      vars:
        minio_cluster: minio
        node_cluster: minio
        minio_data: '/data'                 # it should be a raw device for a multi-node deployment!
        minio_node: '${minio_cluster}-${minio_seq}.pigsty' # minio node name pattern
        haproxy_services:                   # expose minio service via haproxy node port 9002
          - name: minio                     # [REQUIRED] service name, unique
            port: 9002                      # [REQUIRED] service port, unique
            options:                        # [OPTIONAL] minio health check
              - option httpchk
              - option http-keep-alive
              - http-check send meth OPTIONS uri /minio/health/live
              - http-check expect status 200
            servers:
              - { name: minio-1 ,ip: 10.10.10.17 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' }
              - { name: minio-2 ,ip: 10.10.10.18 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' }
              - { name: minio-3 ,ip: 10.10.10.19 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' }


    #==========================================================#
    # etcd: 5 nodes used as dedicated minio cluster
    #==========================================================#
    # ./node.yml -l etcd;  ./etcd.yml -l etcd;
    etcd:  # the 5 node etcd dcs cluster
      hosts:
        10.10.10.31: { etcd_seq: 1 , nodename: etcd-1 }
        10.10.10.32: { etcd_seq: 2 , nodename: etcd-2 }
        10.10.10.33: { etcd_seq: 3 , nodename: etcd-3 }
        10.10.10.34: { etcd_seq: 4 , nodename: etcd-4 }
        10.10.10.35: { etcd_seq: 5 , nodename: etcd-5 }
      vars:
        etcd_cluster: etcd
        node_cluster: etcd


    #==========================================================#
    # pg-meta: reuse infra node as meta cmdb
    #==========================================================#
    # ./pgsql.yml -l pg-meta
    pg-meta:
      hosts:
        10.10.10.11: { pg_seq: 1 , pg_role: primary }
        10.10.10.12: { pg_seq: 2 , pg_role: replica }
        10.10.10.13: { pg_seq: 3 , pg_role: replica }
      vars:
        pg_cluster: pg-meta
        pg_vip_enabled: true
        pg_vip_address: 10.10.10.2/24
        pg_vip_interface: eth1
        pg_users:
          - { name: dbuser_meta ,password: DBUser.Meta   ,pgbouncer: true ,roles: [ dbrole_admin ]    ,comment: pigsty admin user }
          - { name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [ dbrole_readonly ] ,comment: read-only viewer for meta database }
        pg_databases:
          - { name: meta ,baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [ pigsty ] ,extensions: [ { name: postgis, schema: public }, { name: timescaledb } ] }
        pg_hba_rules:
          - { user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes' }
        node_crontab:  # make a full backup on monday 1am, and an incremental backup during weekdays
          - '00 01 * * 1 postgres /pg/bin/pg-backup full'
          - '00 01 * * 2,3,4,5,6,7 postgres /pg/bin/pg-backup'


    #==========================================================#
    # pg-test: dedicate 4 node testing cluster
    #==========================================================#
    # ./pgsql.yml -l pg-test
    pg-test:
      hosts:
        10.10.10.41: { pg_seq: 1 ,pg_role: primary }
        10.10.10.42: { pg_seq: 2 ,pg_role: replica }
        10.10.10.43: { pg_seq: 3 ,pg_role: replica }
        10.10.10.44: { pg_seq: 4 ,pg_role: replica }
      vars:
        pg_cluster: pg-test
        pg_vip_enabled: true
        pg_vip_address: 10.10.10.3/24
        pg_vip_interface: eth1
        pg_users:  [{ name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] }]
        pg_databases: [{ name: test }]


    #==========================================================#
    # pg-src: dedicate 3 node testing cluster, version 14
    #==========================================================#
    # ./pgsql.yml -l pg-src
    pg-src:
      hosts:
        10.10.10.45: { pg_seq: 1 ,pg_role: primary }
        10.10.10.46: { pg_seq: 2 ,pg_role: replica }
        10.10.10.47: { pg_seq: 3 ,pg_role: replica }
      vars:
        pg_cluster: pg-src
        pg_version: 14
        pg_vip_enabled: true
        pg_vip_address: 10.10.10.4/24
        pg_vip_interface: eth1
        node_hugepage_ratio: 0.3
        pg_users:  [{ name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] }]
        pg_databases: [{ name: src }]


    #==========================================================#
    # pg-dst: dedicate 2 node testing cluster
    #==========================================================#
    # ./pgsql.yml -l pg-dst
    pg-dst:
      hosts:
        10.10.10.48: { pg_seq: 1 ,pg_role: primary } # 8C 8G
        10.10.10.49: { pg_seq: 2 ,pg_role: replica } # 1C 2G
      vars:
        pg_cluster: pg-dst
        pg_vip_enabled: true
        pg_vip_address: 10.10.10.5/24
        pg_vip_interface: eth1
        pg_users: [ { name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] } ]
        pg_databases: [ { name: dst } ]


    #==========================================================#
    # pg-citus: 10 node citus cluster (5 x 1p1s)
    #==========================================================#
    pg-citus0: # citus coordinator, pg_group = 0
      hosts:
        10.10.10.50: { pg_seq: 1, pg_role: primary }
        10.10.10.51: { pg_seq: 2, pg_role: replica }
      vars:
        pg_cluster: pg-citus0
        pg_group: 0
        pg_mode: citus                    # pgsql cluster mode: citus
        pg_shard: pg-citus                # citus shard name: pg-citus
        patroni_citus_db: meta            # citus distributed database name
        pg_dbsu_password: DBUser.Postgres # all dbsu password access for citus cluster
        pg_vip_enabled: true
        pg_vip_address: 10.10.10.60/24
        pg_vip_interface: eth1
        pg_libs: 'citus, timescaledb, pg_stat_statements, auto_explain' # citus will be added by patroni automatically
        pg_users: [{ name: citus ,password: citus ,pgbouncer: true ,roles: [ dbrole_admin ] }]
        pg_databases:  [{ name: data ,owner: citus ,extensions: [{name: citus}, {name: postgis}, {name: timescaledb}] }]
        pg_hba_rules:
          - { user: 'all' ,db: all  ,addr: 10.10.10.0/24 ,auth: trust ,title: 'trust citus cluster members'        }
          - { user: 'all' ,db: all  ,addr: 127.0.0.1/32  ,auth: ssl   ,title: 'all user ssl access from localhost' }
          - { user: 'all' ,db: all  ,addr: intra         ,auth: ssl   ,title: 'all user ssl access from intranet'  }

    pg-citus1: # citus data node 1, pg_group = 1
      hosts:
        10.10.10.52: { pg_seq: 1, pg_role: primary }
        10.10.10.53: { pg_seq: 2, pg_role: replica }
      vars:
        pg_cluster: pg-citus1
        pg_group: 1
        pg_mode: citus                    # pgsql cluster mode: citus
        pg_shard: pg-citus                # citus shard name: pg-citus
        patroni_citus_db: meta            # citus distributed database name
        pg_dbsu_password: DBUser.Postgres # all dbsu password access for citus cluster
        pg_vip_enabled: true
        pg_vip_address: 10.10.10.61/24
        pg_vip_interface: eth1
        pg_libs: 'citus, timescaledb, pg_stat_statements, auto_explain' # citus will be added by patroni automatically
        pg_users: [{ name: citus ,password: citus ,pgbouncer: true ,roles: [ dbrole_admin ] }]
        pg_databases:  [{ name: data ,owner: citus ,extensions: [{name: citus}, {name: postgis}, {name: timescaledb}] }]
        pg_hba_rules:
          - { user: 'all' ,db: all  ,addr: 10.10.10.0/24 ,auth: trust ,title: 'trust citus cluster members'        }
          - { user: 'all' ,db: all  ,addr: 127.0.0.1/32  ,auth: ssl   ,title: 'all user ssl access from localhost' }
          - { user: 'all' ,db: all  ,addr: intra         ,auth: ssl   ,title: 'all user ssl access from intranet'  }

    pg-citus2: # citus data node 2, pg_group = 2
      hosts:
        10.10.10.54: { pg_seq: 1, pg_role: primary }
        10.10.10.55: { pg_seq: 2, pg_role: replica }
      vars:
        pg_cluster: pg-citus2
        pg_group: 2
        pg_mode: citus                    # pgsql cluster mode: citus
        pg_shard: pg-citus                # citus shard name: pg-citus
        patroni_citus_db: meta            # citus distributed database name
        pg_dbsu_password: DBUser.Postgres # all dbsu password access for citus cluster
        pg_vip_enabled: true
        pg_vip_address: 10.10.10.62/24
        pg_vip_interface: eth1
        pg_libs: 'citus, timescaledb, pg_stat_statements, auto_explain' # citus will be added by patroni automatically
        pg_users: [{ name: citus ,password: citus ,pgbouncer: true ,roles: [ dbrole_admin ] }]
        pg_databases:  [{ name: data ,owner: citus ,extensions: [{name: citus}, {name: postgis}, {name: timescaledb}] }]
        pg_hba_rules:
          - { user: 'all' ,db: all  ,addr: 10.10.10.0/24 ,auth: trust ,title: 'trust citus cluster members'        }
          - { user: 'all' ,db: all  ,addr: 127.0.0.1/32  ,auth: ssl   ,title: 'all user ssl access from localhost' }
          - { user: 'all' ,db: all  ,addr: intra         ,auth: ssl   ,title: 'all user ssl access from intranet'  }

    pg-citus3: # citus data node 3, pg_group = 3
      hosts:
        10.10.10.56: { pg_seq: 1, pg_role: primary }
        10.10.10.57: { pg_seq: 2, pg_role: replica }
      vars:
        pg_cluster: pg-citus3
        pg_group: 3
        pg_mode: citus                    # pgsql cluster mode: citus
        pg_shard: pg-citus                # citus shard name: pg-citus
        patroni_citus_db: meta            # citus distributed database name
        pg_dbsu_password: DBUser.Postgres # all dbsu password access for citus cluster
        pg_vip_enabled: true
        pg_vip_address: 10.10.10.63/24
        pg_vip_interface: eth1
        pg_libs: 'citus, timescaledb, pg_stat_statements, auto_explain' # citus will be added by patroni automatically
        pg_users: [{ name: citus ,password: citus ,pgbouncer: true ,roles: [ dbrole_admin ] }]
        pg_databases:  [{ name: data ,owner: citus ,extensions: [{name: citus}, {name: postgis}, {name: timescaledb}] }]
        pg_hba_rules:
          - { user: 'all' ,db: all  ,addr: 10.10.10.0/24 ,auth: trust ,title: 'trust citus cluster members'        }
          - { user: 'all' ,db: all  ,addr: 127.0.0.1/32  ,auth: ssl   ,title: 'all user ssl access from localhost' }
          - { user: 'all' ,db: all  ,addr: intra         ,auth: ssl   ,title: 'all user ssl access from intranet'  }

    pg-citus4: # citus data node 4, pg_group = 4
      hosts:
        10.10.10.58: { pg_seq: 1, pg_role: primary }
        10.10.10.59: { pg_seq: 2, pg_role: replica }
      vars:
        pg_cluster: pg-citus4
        pg_group: 4
        pg_mode: citus                    # pgsql cluster mode: citus
        pg_shard: pg-citus                # citus shard name: pg-citus
        patroni_citus_db: meta            # citus distributed database name
        pg_dbsu_password: DBUser.Postgres # all dbsu password access for citus cluster
        pg_vip_enabled: true
        pg_vip_address: 10.10.10.64/24
        pg_vip_interface: eth1
        pg_libs: 'citus, timescaledb, pg_stat_statements, auto_explain' # citus will be added by patroni automatically
        pg_users: [{ name: citus ,password: citus ,pgbouncer: true ,roles: [ dbrole_admin ] }]
        pg_databases:  [{ name: data ,owner: citus ,extensions: [{name: citus}, {name: postgis}, {name: timescaledb}] }]
        pg_hba_rules:
          - { user: 'all' ,db: all  ,addr: 10.10.10.0/24 ,auth: trust ,title: 'trust citus cluster members'        }
          - { user: 'all' ,db: all  ,addr: 127.0.0.1/32  ,auth: ssl   ,title: 'all user ssl access from localhost' }
          - { user: 'all' ,db: all  ,addr: intra         ,auth: ssl   ,title: 'all user ssl access from intranet'  }


    #==========================================================#
    # pg-v12 - v16
    #==========================================================#
    pg-v12:
      hosts: { 10.10.10.82: { pg_seq: 1 ,pg_role: primary } }
      vars:
        pg_cluster: pg-v12
        pg_version: 12
        pg_libs: 'pg_stat_statements, auto_explain'
        pg_packages:
          - postgresql12* wal2json_12* pg_repack_12* passwordcheck_cracklib_12* pgbouncer pg_exporter pgbadger vip-manager patroni patroni-etcd pgbackrest
        pg_extensions: []
        pg_users: [ { name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] } ]
        pg_databases: [ { name: v12 } ]

    pg-v13:
      hosts: { 10.10.10.83: { pg_seq: 1 ,pg_role: primary } }
      vars:
        pg_cluster: pg-v13
        pg_version: 13
        pg_libs: 'pg_stat_statements, auto_explain'
        pg_packages:
          - postgresql13* wal2json_13* pg_repack_13* passwordcheck_cracklib_13* pgbouncer pg_exporter pgbadger vip-manager patroni patroni-etcd pgbackrest
        pg_extensions: []
        pg_users: [ { name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] } ]
        pg_databases: [ { name: v13 } ]

    pg-v14:
      hosts: { 10.10.10.84: { pg_seq: 1 ,pg_role: primary } }
      vars:
        pg_cluster: pg-v14
        pg_version: 14
        pg_users: [ { name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] } ]
        pg_databases: [ { name: v14 } ]

    pg-v15:
      hosts: { 10.10.10.85: { pg_seq: 1 ,pg_role: primary } }
      vars:
        pg_cluster: pg-v15
        pg_version: 15
        pg_users: [ { name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] } ]
        pg_databases: [ { name: v15 } ]

    pg-v16:
      hosts: { 10.10.10.86: { pg_seq: 1 ,pg_role: primary } }
      vars:
        pg_cluster: pg-v16
        pg_version: 16
        pg_libs: 'pg_stat_statements, auto_explain'
        pg_packages:
          - postgresql16* pgbouncer pg_exporter pgbadger vip-manager patroni patroni-etcd pgbackrest
        pg_extensions: [ ]
        pg_users: [ { name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] } ]
        pg_databases: [ { name: v16 } ]


    #==========================================================#
    # redis-meta: reuse the 5 etcd nodes as redis sentinel
    #==========================================================#
    # ./redis.yml -l redis-meta
    redis-meta:
      hosts:
        10.10.10.31: { redis_node: 1 , redis_instances: { 6001: {} } }
        10.10.10.32: { redis_node: 2 , redis_instances: { 6001: {} } }
        10.10.10.33: { redis_node: 3 , redis_instances: { 6001: {} } }
        10.10.10.34: { redis_node: 4 , redis_instances: { 6001: {} } }
        10.10.10.35: { redis_node: 5 , redis_instances: { 6001: {} } }
      vars:
        redis_cluster: redis-meta
        redis_password: 'redis.meta'
        redis_mode: sentinel
        redis_max_memory: 512MB


    #==========================================================#
    # redis-src: reuse pg-src 3 nodes for redis
    #==========================================================#
    # ./redis.yml -l redis-src
    redis-src:
      hosts:
        10.10.10.45: { redis_node: 1 , redis_instances: {6501: {  }                               }}
        10.10.10.46: { redis_node: 2 , redis_instances: {6501: { replica_of: '10.10.10.45 6501' } }}
        10.10.10.47: { redis_node: 3 , redis_instances: {6501: { replica_of: '10.10.10.45 6501' } }}
      vars:
        redis_cluster: redis-src
        redis_password: 'redis.src'
        redis_max_memory: 128MB

    #==========================================================#
    # redis-dst: reuse pg-dst 2 nodes for redis
    #==========================================================#
    # ./redis.yml -l redis-dst
    redis-dst:
      hosts:
        10.10.10.48: { redis_node: 1 , redis_instances: {6501: {  }                               }}
        10.10.10.49: { redis_node: 2 , redis_instances: {6501: { replica_of: '10.10.10.48 6501' } }}
      vars:
        redis_cluster: redis-dst
        redis_password: 'redis.dst'
        redis_max_memory: 128MB

    #==========================================================#
    # redis-test: redis native cluster in 6 nodes, 24 instances
    #==========================================================#
    # ./node.yml -l redis-test; ./redis.yml -l redis-test
    redis-test:
      hosts:
        10.10.10.71: { redis_node: 1 ,redis_instances: { 6501: {} ,6502: {} ,6503: {} } }
        10.10.10.72: { redis_node: 2 ,redis_instances: { 6501: {} ,6502: {} ,6503: {} } }
        10.10.10.73: { redis_node: 3 ,redis_instances: { 6501: {} ,6502: {} ,6503: {} } }
        10.10.10.74: { redis_node: 4 ,redis_instances: { 6501: {} ,6502: {} ,6503: {} } }
        10.10.10.75: { redis_node: 5 ,redis_instances: { 6501: {} ,6502: {} ,6503: {} } }
        10.10.10.76: { redis_node: 6 ,redis_instances: { 6501: {} ,6502: {} ,6503: {} } }
      vars:
        redis_cluster: redis-test
        redis_password: 'redis.test'
        redis_mode: cluster
        redis_max_memory: 1000MB
        node_cluster: redis-test


    #==========================================================#
    # admin: reuse bare metal node
    #==========================================================#
    # admin:  # the bare metal host
    #   hosts: { 10.10.10.1: {} }
    #   vars: { dns_enabled: false, admin_ip: 10.10.10.1 }

    # pg-admin: # the postgres on bare metal host
    #   hosts: { 10.10.10.1: { pg_seq: 1 , pg_role: primary } }
    #   vars:
    #     pg_cluster: pg-meta       # this is the admin cmdb for convenience
    #     pg_conf: olap.yml         # we want to use olap config for this database
    #     node_conf: oltp           # this is a powerful machine, use oltp config
    #     patroni_mode: remove      # this singleton testing db does not need patroni HA
    #     pgbackrest_enabled: false # do not back up this testing postgres instance
    #     haproxy_enabled: false    # do not install haproxy for testing database
    #     pg_users:
    #       - { name: dbuser_meta ,password: DBUser.Meta   ,pgbouncer: true ,roles: [ dbrole_admin ]    ,comment: pigsty admin user }
    #       - { name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [ dbrole_readonly ] ,comment: read-only viewer for meta database }
    #     pg_databases:
    #       - { name: meta ,baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [ pigsty ] ,extensions: [ { name: postgis, schema: public }, { name: timescaledb } ] }
    #     pg_hba_rules:
    #       - { user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes' }



  #============================================================#
  # Global Variables
  #============================================================#
  vars:

    #==========================================================#
    # INFRA
    #==========================================================#
    version: v2.2.0                   # pigsty version string
    admin_ip: 10.10.10.11             # admin node ip address
    region: china                     # upstream mirror region: default|china|europe
    infra_portal:                     # domain names and upstream servers
      home         : { domain: h.pigsty }
      grafana      : { domain: g.pigsty ,endpoint: "10.10.10.11:3000" , websocket: true }
      prometheus   : { domain: p.pigsty ,endpoint: "10.10.10.11:9090" }
      alertmanager : { domain: a.pigsty ,endpoint: "10.10.10.11:9093" }
      blackbox     : { endpoint: "10.10.10.11:9115" }
      loki         : { endpoint: "10.10.10.11:3100" }
      minio        : { domain: sss.pigsty  ,endpoint: "10.10.10.18:9001" ,scheme: https ,websocket: true }
    nginx_navbar: []
    dns_records:                      # dynamic dns records resolved by dnsmasq
      - 10.10.10.1 h.pigsty a.pigsty p.pigsty g.pigsty

    #==========================================================#
    # NODE
    #==========================================================#
    node_conf: tiny # use rocky9 local repo as node default
    node_repo_local_urls: ['http://10.10.10.11/pigsty.repo']
    node_timezone: Asia/Hong_Kong     # use Asia/Hong_Kong Timezone
    node_dns_servers:                 # DNS servers in /etc/resolv.conf
      - 10.10.10.11
      - 10.10.10.12
      - 10.10.10.13
    node_etc_hosts:
      - 10.10.10.1 h.pigsty a.pigsty p.pigsty g.pigsty
      - 10.10.10.17 sss.pigsty
    node_ntp_servers:                 # NTP servers in /etc/chrony.conf
      - pool cn.pool.ntp.org iburst
      - pool 10.10.10.1 iburst
    docker_registry_mirrors:          # docker mirror in mainland china
      - "https://hub-mirror.c.163.com"
      - "https://mirror.baidubce.com"

    #==========================================================#
    # PGSQL
    #==========================================================#
    pg_conf: tiny.yml
    pgbackrest_method: minio          # USE THE HA MINIO THROUGH A LOAD BALANCER
    pgbackrest_repo:                  # pgbackrest repo: https://pgbackrest.org/configuration.html#section-repository
      local:                          # default pgbackrest repo with local posix fs
        path: /pg/backup              # local backup directory, `/pg/backup` by default
        retention_full_type: count    # retention full backups by count
        retention_full: 2             # keep 2, at most 3 full backup when using local fs repo
      minio:
        type: s3
        s3_endpoint: sss.pigsty       # s3_endpoint could be any load balancer: 10.10.10.1{0,1,2}, or domain names point to any of the 3 nodes
        s3_region: us-east-1          # you could use external domain name: sss.pigsty , which resolve to any members  (`minio_domain`)
        s3_bucket: pgsql              # instance & nodename can be used : minio-1.pigsty minio-1.pigsty minio-1.pigsty minio-1 minio-2 minio-3
        s3_key: pgbackrest            # Better using a new password for MinIO pgbackrest user
        s3_key_secret: S3User.Backup
        s3_uri_style: path
        path: /pgbackrest
        storage_port: 9002            # Use the load balancer port 9002 instead of default 9000 (direct access)
        storage_ca_file: /etc/pki/ca.crt
        bundle: y
        cipher_type: aes-256-cbc      # Better using a new cipher password for your production environment
        cipher_pass: pgBackRest.${pg_cluster}
        retention_full_type: time
        retention_full: 14


...