---
######################################################################
# File      :   pigsty.yml
# Desc      :   Pigsty 4-node demo (public demo)
# Link      :   https://pigsty.cc/#/v-config
# Ctime     :   2020-05-22
# Mtime     :   2021-11-03
# Copyright (C) 2018-2021 Ruohang Feng (rh@vonng.com)
######################################################################


######################################################################
#                        Sandbox (4-node)                            #
#====================================================================#
# admin user : vagrant  (nopass ssh & sudo already set)              #
# 1.  meta    :    172.21.0.11     (2 Core | 4GB)    pg-meta         #
# 2.  node-1  :    172.21.0.3      (1 Core | 1GB)    pg-test-1       #
# 3.  node-2  :    172.21.0.4      (1 Core | 1GB)    pg-test-2       #
# 4.  node-3  :    172.21.0.16     (1 Core | 1GB)    pg-test-3       #
######################################################################


all: # top-level namespace

  #==================================================================#
  #                           Clusters                                #
  #==================================================================#
  # postgres database clusters are defined as kv pair in `all.children`
  # where the key is cluster name and the value is the object consist
  # of cluster members (hosts) and cluster specific variables (vars)
  # meta nodes are defined in special group "meta" with `meta_node=true`

  children:

    #----------------------------------#
    # meta nodes (admin controller)    #
    #----------------------------------#
    meta:
      vars: { meta_node: true , ansible_group_priority: 99 }
      hosts:
        172.21.0.11: { }

    #----------------------------------#
    # cluster: pg-meta (default cmdb)
    #----------------------------------#
    pg-meta:
      hosts:
        172.21.0.11: { pg_seq: 1, pg_role: primary , pg_offline_query: true }
      vars:
        pg_cluster: pg-meta
        pg_conf: small.yml
        node_tune: oltp
        pg_users:
          - { name: dbuser_meta      , password: DBUser.Meta       , pgbouncer: true , roles: [ dbrole_admin ] , comment: pigsty admin user }
          - { name: dbuser_view      , password: DBUser.Viewer     , pgbouncer: true , roles: [ dbrole_readonly ] , comment: read-only viewer for meta database }
          - {name: dbuser_grafana    , password: DBUser.Grafana    , pgbouncer: true , roles: [dbrole_admin], comment: admin user for grafana database }
          - {name: dbuser_prometheus , password: DBUser.Prometheus , pgbouncer: true , roles: [dbrole_admin], comment: admin user for prometheus database , createrole: true }
        pg_databases:
          - name: meta
            baseline: cmdb.sql
            comment: pigsty meta database
            connlimit: -1
            schemas: [ pigsty ]
            extensions:
              - { name: adminpack, schema: pg_catalog }
              - { name: postgis, schema: public }
              - { name: timescaledb }

          - { name: grafana,    owner: dbuser_grafana    , revokeconn: true , comment: grafana    primary database }
          - { name: prometheus, owner: dbuser_prometheus , revokeconn: true , comment: prometheus primary database , extensions: [{ name: timescaledb }]}


    #----------------------------------#
    # cluster: pg-test (3-node demo)   #
    #----------------------------------#

    pg-test:                                # define the new 3-node cluster pg-test
      hosts:
        172.21.0.3:  { pg_seq: 1, pg_role: primary }
        172.21.0.4:  { pg_seq: 2, pg_role: replica }
        172.21.0.16: { pg_seq: 3, pg_role: replica, pg_offline_query: true }

      vars:
        pg_cluster: pg-test
        pg_users: [{name: test , password: test  ,pgbouncer: true ,roles: [dbrole_admin], comment: test user for test database cluster }]
        pg_databases: [{ name: test }]      # create a database and user named 'test'

        pg_services_extra:
          - name: standby                   # required, service name, the actual svc name will be prefixed with `pg_cluster`, e.g: pg-meta-standby
            src_ip: "*"                     # required, service bind ip address, `*` for all ip, `vip` for cluster `vip_address`
            src_port: 5435                  # required, service exposed port (work as kubernetes service node port mode)
            dst_port: postgres              # optional, destination port, postgres|pgbouncer|<port_number>   , pgbouncer(6432) by default
            check_method: http              # optional, health check method: http is the only available method for now
            check_port: patroni             # optional, health check port: patroni|pg_exporter|<port_number> , patroni(8008) by default
            check_url: /read-only?lag=0     # optional, health check url path, / by default
            check_code: 200                 # optional, health check expected http code, 200 by default
            selector: "[]"                  # required, JMESPath to filter inventory ()
            selector_backup: "[? pg_role == `primary`]"  # primary used as backup server for standby service (will not work because /sync for )
            haproxy:                        # optional, adhoc parameters for haproxy service provider (vip_l4 is another service provider)
              maxconn: 3000                 # optional, max allowed front-end connection
              balance: roundrobin           # optional, haproxy load balance algorithm (roundrobin by default, other: leastconn)
              default_server_options: 'inter 3s fastinter 1s downinter 5s rise 3 fall 3 on-marked-down shutdown-sessions slowstart 30s maxconn 3000 maxqueue 128 weight 100'




  #==================================================================#
  #                           Globals                                #
  #==================================================================#
  vars:

    #------------------------------------------------------------------------------
    # CONNECTION PARAMETERS
    #------------------------------------------------------------------------------
    # this section defines connection parameters (How to perform ssh sudo on nodes)

    # ansible_user: vagrant                       # admin user with ssh access and sudo privilege
    # ansible_password: <remote ssh pass>         # admin user's ssh password (sshpass required, not recommended)
    # ansible_become_pass: <remote sudo password> # admin user's sudo password (security breach, not recommended)

    proxy_env:                                    # global proxy env when downloading packages
      no_proxy: "localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,*.pigsty,*.aliyun.com,mirrors.*,*.myqcloud.com"
      # http_proxy:  # set your proxy here: e.g http://user:pass@proxy.xxx.com
      # https_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com
      # all_proxy:   # set your proxy here: e.g http://user:pass@proxy.xxx.com


    #------------------------------------------------------------------------------
    # REPO PROVISION
    #------------------------------------------------------------------------------
    # this section describes pigsty local yum repo

    # - repo basic - #
    repo_enabled: true                            # build local yum repo on meta nodes?
    repo_name: pigsty                             # local repo name (do not change)
    repo_address: pigsty                          # repo external address (ip:port or url)
    repo_port: 80                                 # listen address, must same as repo_address
    repo_home: /www                               # default repo dir location
    repo_rebuild: false                           # force re-download packages
    repo_remove: true                             # remove existing repos

    # - where to download - #
    repo_upstreams:
      - name: base
        description: CentOS-$releasever - Base
        gpgcheck: no
        baseurl:
          - https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/os/$basearch/ # tuna
          - http://mirrors.aliyun.com/centos/$releasever/os/$basearch/
          - http://mirrors.aliyuncs.com/centos/$releasever/os/$basearch/
          - http://mirrors.cloud.aliyuncs.com/centos/$releasever/os/$basearch/    # aliyun
          - http://mirror.centos.org/centos/$releasever/os/$basearch/             # official

      - name: updates
        description: CentOS-$releasever - Updates
        gpgcheck: no
        baseurl:
          - https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/updates/$basearch/ # tuna
          - http://mirrors.aliyun.com/centos/$releasever/updates/$basearch/
          - http://mirrors.aliyuncs.com/centos/$releasever/updates/$basearch/
          - http://mirrors.cloud.aliyuncs.com/centos/$releasever/updates/$basearch/    # aliyun
          - http://mirror.centos.org/centos/$releasever/updates/$basearch/             # official

      - name: extras
        description: CentOS-$releasever - Extras
        baseurl:
          - https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/extras/$basearch/ # tuna
          - http://mirrors.aliyun.com/centos/$releasever/extras/$basearch/
          - http://mirrors.aliyuncs.com/centos/$releasever/extras/$basearch/
          - http://mirrors.cloud.aliyuncs.com/centos/$releasever/extras/$basearch/    # aliyun
          - http://mirror.centos.org/centos/$releasever/extras/$basearch/             # official
        gpgcheck: no

      - name: epel
        description: CentOS $releasever - epel
        gpgcheck: no
        baseurl:
          - https://mirrors.tuna.tsinghua.edu.cn/epel/$releasever/$basearch   # tuna
          - http://mirrors.aliyun.com/epel/$releasever/$basearch              # aliyun
          - http://download.fedoraproject.org/pub/epel/$releasever/$basearch  # official

      - name: grafana
        description: Grafana
        enabled: yes
        gpgcheck: no
        baseurl:
          - https://mirrors.tuna.tsinghua.edu.cn/grafana/yum/rpm    # tuna mirror
          - https://packages.grafana.com/oss/rpm                    # official

      - name: prometheus
        description: Prometheus and exporters
        gpgcheck: no
        baseurl: https://packagecloud.io/prometheus-rpm/release/el/$releasever/$basearch # no other mirrors, quite slow

      - name: pgdg-common
        description: PostgreSQL common RPMs for RHEL/CentOS $releasever - $basearch
        gpgcheck: no
        baseurl:
          - http://mirrors.tuna.tsinghua.edu.cn/postgresql/repos/yum/common/redhat/rhel-$releasever-$basearch  # tuna
          - https://download.postgresql.org/pub/repos/yum/common/redhat/rhel-$releasever-$basearch             # official

      - name: pgdg14
        description: PostgreSQL 14 for RHEL/CentOS $releasever - $basearch
        gpgcheck: no
        baseurl:
          - https://mirrors.tuna.tsinghua.edu.cn/postgresql/repos/yum/14/redhat/rhel-$releasever-$basearch    # tuna
          - https://download.postgresql.org/pub/repos/yum/14/redhat/rhel-$releasever-$basearch                # official

      - name: timescaledb
        description: PostgreSQL 13 for RHEL/CentOS $releasever - $basearch
        gpgcheck: no
        baseurl:
          - https://packagecloud.io/timescale/timescaledb/el/7/$basearch

      - name: centos-sclo
        description: CentOS-$releasever - SCLo
        gpgcheck: no
        baseurl: # mirrorlist: http://mirrorlist.centos.org?arch=$basearch&release=$releasever&repo=sclo-sclo
          - http://mirrors.aliyun.com/centos/$releasever/sclo/$basearch/sclo/
          - http://repo.virtualhosting.hk/centos/$releasever/sclo/$basearch/sclo/

      - name: centos-sclo-rh
        description: CentOS-$releasever - SCLo rh
        gpgcheck: no
        baseurl: # mirrorlist: http://mirrorlist.centos.org?arch=$basearch&release=7&repo=sclo-rh
          - http://mirrors.aliyun.com/centos/$releasever/sclo/$basearch/rh/
          - http://repo.virtualhosting.hk/centos/$releasever/sclo/$basearch/rh/

      - name: nginx
        description: Nginx Official Yum Repo
        skip_if_unavailable: true
        gpgcheck: no
        baseurl: http://nginx.org/packages/centos/$releasever/$basearch/

      - name: haproxy
        description: Copr repo for haproxy
        skip_if_unavailable: true
        gpgcheck: no
        baseurl: https://download.copr.fedorainfracloud.org/results/roidelapluie/haproxy/epel-$releasever-$basearch/

      # for latest consul & kubernetes
      - name: harbottle
        description: Copr repo for main owned by harbottle
        skip_if_unavailable: true
        gpgcheck: no
        baseurl: https://download.copr.fedorainfracloud.org/results/harbottle/main/epel-$releasever-$basearch/


    # - what to download - #
    repo_packages:
      # repo bootstrap packages
      - epel-release nginx wget yum-utils yum createrepo sshpass unzip                        # bootstrap packages

      # node basic packages
      - ntp chrony uuid lz4 nc pv jq vim-enhanced make patch bash lsof wget git tuned         # basic system util
      - readline zlib openssl libyaml libxml2 libxslt perl-ExtUtils-Embed ca-certificates     # basic pg dependency
      - numactl grubby sysstat dstat iotop bind-utils net-tools tcpdump socat ipvsadm telnet  # system utils

      # dcs & monitor packages
      - grafana prometheus2 pushgateway alertmanager                                          # monitor and ui
      - node_exporter postgres_exporter nginx_exporter blackbox_exporter                      # exporter
      - consul consul_exporter consul-template etcd                                           # dcs

      # python3 dependencies
      - ansible python python-pip python-psycopg2 audit                                       # ansible & python
      - python3 python3-psycopg2 python36-requests python3-etcd python3-consul                # python3
      - python36-urllib3 python36-idna python36-pyOpenSSL python36-cryptography               # patroni extra deps

      # proxy and load balancer
      - haproxy keepalived dnsmasq                                                            # proxy and dns

      # postgres common Packages
      - patroni patroni-consul patroni-etcd pgbouncer pg_cli pgbadger pg_activity             # major components
      - pgcenter boxinfo check_postgres emaj pgbconsole pg_bloat_check pgquarrel              # other common utils
      - barman barman-cli pgloader pgFormatter pitrery pspg pgxnclient PyGreSQL pgadmin4 tail_n_mail

      # postgres 14 packages
      - postgresql14*                                                                              # postgresql 14 kernel
      - timescaledb-2-postgresql-14                                                                # timescaledb
      - postgresql14* postgis31_14* citus_14* pg_repack_14 pglogical_14*                           # postgresql 14 extensions
      - pg_qualstats_14 pg_stat_kcache_14 system_stats_14 bgw_replstatus_14 pg_stat_monitor_14     # stats extensions
      - plr_14 plsh_14 plpgsql_check_14 plproxy_14 plr_14 plsh_14 plpgsql_check_14 pldebugger_14   # PL extensions
      - hdfs_fdw_14 mongo_fdw_14 mysql_fdw_14 ogr_fdw_14 pgbouncer_fdw_14 tds_fdw_14 pgmemcache_14 # FDW extensions
      - wal2json_14 count_distinct_14 geoip_14 ip4r_14 pguri_14 hll_14 prefix_14 rum_14 hypopg_14  # Type & Index
      - pg_cron_14 pg_wait_sampling_14 pg_permissions_14 powa_14 pg_track_settings_14 pgaudit16_14 # Misc
      - pgq_14 jsquery_14 logerrors_14 periods_14 pg_auto_failover_14 pg_catcheck_14 pgfincore_14
      - pg_fkpart_14 pg_jobmon_14 pg_partman_14 pg_prioritize_14 safeupdate_14 semver_14
      - pgcryptokey_14 pgexportdoc_14 pgimportdoc_14 pgmp_14 tdigest_14 orafce_14 table_version_14

      # build & devel packages (optional)
      - gcc gcc-c++ clang coreutils diffutils rpm-build rpm-devel rpmlint rpmdevtools
      - zlib-devel openssl-libs openssl-devel libxml2-devel libxslt-devel
      # - pam-devel openldap-devel systemd-devel tcl-devel python-devel

    repo_url_packages:
      - https://github.com/Vonng/pg_exporter/releases/download/v0.4.0/pg_exporter-0.4.0-1.el7.x86_64.rpm            # pg_exporter rpm
      - https://github.com/cybertec-postgresql/vip-manager/releases/download/v1.0.1/vip-manager_1.0.1-1_amd64.rpm   # vip-manager
      - https://github.com/timescale/promscale/releases/download/0.6.2/promscale_0.6.2_Linux_x86_64.rpm             # promscale
      - https://github.com/prometheus/node_exporter/releases/download/v1.2.2/node_exporter-1.2.2.linux-amd64.tar.gz # monitor binaries
      - https://github.com/Vonng/pg_exporter/releases/download/v0.4.0/pg_exporter_v0.4.0_linux-amd64.tar.gz
      - https://github.com/grafana/loki/releases/download/v2.3.0/loki-linux-amd64.zip
      - https://github.com/grafana/loki/releases/download/v2.3.0/promtail-linux-amd64.zip
      - https://github.com/grafana/loki/releases/download/v2.3.0/logcli-linux-amd64.zip
      - https://github.com/grafana/loki/releases/download/v2.3.0/loki-canary-linux-amd64.zip
      - https://github.com/dalibo/pev2/releases/download/v0.23.0/pev2.tar.gz
      - https://github.com/sosedoff/pgweb/releases/download/v0.11.9/pgweb_linux_amd64.zip

    #------------------------------------------------------------------------------
    # NODE PROVISION
    #------------------------------------------------------------------------------
    # this section defines how to provision nodes
    # nodename:                                   # if defined, node's hostname will be overwritten
    # meta_node: false                            # node with meta_node will be marked as admin node

    # - node dns - #
    node_dns_hosts:                               # static dns records in /etc/hosts
      - 172.21.0.11 meta
      - 172.21.0.11 pigsty c.pigsty g.pigsty p.pigsty a.pigsty cli.pigsty lab.pigsty

    node_dns_server: none                         # add (default) | none (skip) | overwrite (remove old settings)
    node_dns_servers:                             # dynamic nameserver in /etc/resolv.conf
      - 172.21.0.11
    node_dns_options:                             # dns resolv options
      - options single-request-reopen timeout:1 rotate
      - domain service.consul

    # - node repo - #
    node_repo_method: local                       # none|local|public (use local repo for production env)
    node_repo_remove: true                        # whether remove existing repo
    node_local_repo_url:                          # local repo url (if method=local, make sure firewall is configured or disabled)
      - http://pigsty/pigsty.repo

    # - node packages - #
    node_packages:                                # common packages for all nodes
      - wget,yum-utils,sshpass,ntp,chrony,tuned,uuid,lz4,vim-minimal,make,patch,bash,lsof,wget,unzip,git,readline,zlib,openssl
      - numactl,grubby,sysstat,dstat,iotop,bind-utils,net-tools,tcpdump,socat,ipvsadm,telnet,tuned,pv,jq
      - python3,python3-psycopg2,python36-requests,python3-etcd,python3-consul
      - python36-urllib3,python36-idna,python36-pyOpenSSL,python36-cryptography
      - node_exporter,consul,consul-template,etcd,haproxy,keepalived,vip-manager
    node_extra_packages:                          # extra packages for all nodes
      - patroni,patroni-consul,patroni-etcd,pgbouncer,pgbadger,pg_activity
    node_meta_packages:                           # packages for meta nodes only
      - grafana,prometheus2,alertmanager,nginx_exporter,blackbox_exporter,pushgateway
      - nginx,ansible,pgbadger,python-psycopg2,dnsmasq
      - gcc,gcc-c++,clang,coreutils,diffutils,rpm-build,rpm-devel,rpmlint,rpmdevtools
      - zlib-devel,openssl-libs,openssl-devel,libxml2-devel,libxslt-devel
    node_meta_pip_install: 'jupyterlab'           # pip packages installed on meta


    # - node features - #
    node_disable_numa: false                      # disable numa, important for production database, reboot required
    node_disable_swap: false                      # disable swap, important for production database
    node_disable_firewall: true                   # disable firewall (required if using kubernetes)
    node_disable_selinux: true                    # disable selinux  (required if using kubernetes)
    node_static_network: true                     # keep dns resolver settings after reboot
    node_disk_prefetch: false                     # setup disk prefetch on HDD to increase performance

    # - node kernel modules - #
    node_kernel_modules: [softdog, br_netfilter, ip_vs, ip_vs_rr, ip_vs_rr, ip_vs_wrr, ip_vs_sh]

    # - node tuned - #
    node_tune: tiny                               # install and activate tuned profile: none|oltp|olap|crit|tiny
    node_sysctl_params: { }                       # set additional sysctl parameters, k:v format
    # net.bridge.bridge-nf-call-iptables: 1       # example sysctl parameters

    # - node admin - #
    node_admin_setup: true                        # create a default admin user defined by `node_admin_*` ?
    node_admin_uid: 88                            # uid and gid for this admin user
    node_admin_username: dba                      # name of this admin user, dba by default
    node_admin_ssh_exchange: true                 # exchange admin ssh key among each pgsql cluster ?
    node_admin_pk_current: true                   # add current user's ~/.ssh/id_rsa.pub to admin authorized_keys ?
    node_admin_pks:                               # ssh public keys to be added to admin user (REPLACE WITH YOURS!)
      - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAAAgQC7IMAMNavYtWwzAJajKqwdn3ar5BhvcwCnBTxxEkXhGlCO2vfgosSAQMEflfgvkiI5nM1HIFQ8KINlx1XLO7SdL5KdInG5LIJjAFh0pujS4kNCT9a5IGvSq1BrzGqhbEcwWYdju1ZPYBcJm/MG+JD0dYCh8vfrYB/cYMD0SOmNkQ== vagrant@pigsty.com'

    # - node tz - #
    node_timezone: Asia/Hong_Kong                 # default node timezone, empty will not change

    # - node ntp - #
    node_ntp_config: false                         # config ntp service? false will leave it with system default
    node_ntp_service: ntp                         # ntp service provider: ntp|chrony
    node_ntp_servers:                             # default NTP servers
      - pool cn.pool.ntp.org iburst
      - pool pool.ntp.org iburst
      - pool time.pool.aliyun.com iburst
      - server 172.21.0.11 iburst
      - server ntp.tuna.tsinghua.edu.cn iburst


    #------------------------------------------------------------------------------
    # META PROVISION
    #------------------------------------------------------------------------------
    # - ca - #
    ca_method: create                             # create|copy|recreate
    ca_subject: "/CN=root-ca"                     # self-signed CA subject
    ca_homedir: /ca                               # ca cert directory
    ca_cert: ca.crt                               # ca public key/cert
    ca_key: ca.key                                # ca private key

    # - nginx - #
    nginx_upstream:                               # domain names that will be used for accessing pigsty services
      - { name: home,          domain: home.pigsty.cc,    endpoint: "172.21.0.11:80" }     # default -> index.html (80)
      - { name: grafana,       domain: demo.pigsty.cc,    endpoint: "172.21.0.11:3000" }   # pigsty grafana (3000)
      - { name: prometheus,    domain: p.pigsty.cc,        endpoint: "172.21.0.11:9090" }   # pigsty prometheus (9090)
      - { name: alertmanager,  domain: a.pigsty.cc,        endpoint: "172.21.0.11:9093" }   # pigsty alertmanager (9093)
      # some service can only be accessed via domain name due to security reasons (e.g consul, pgweb, jupyter)
      - { name: consul,        domain: c.pigsty.cc,        endpoint: "127.0.0.1:8500" }     # pigsty consul UI (8500) (domain required)
      - { name: pgweb,         domain: cli.pigsty.cc,      endpoint: "127.0.0.1:8081" }     # pgweb console (8081)
      - { name: jupyter,       domain: lab.pigsty.cc,      endpoint: "127.0.0.1:8888" }     # jupyter lab (8888)

    # - app - #
    app_list:                                      # show extra application links on home page
      - { name: Pev2    , url : '/pev2'        , comment: 'postgres explain visualizer 2' }
      - { name: Logs    , url : '/logs'        , comment: 'realtime pgbadger log sample' }
      - { name: Report  , url : '/report'      , comment: 'daily log summary report ' }
      - { name: Pkgs    , url : '/pigsty'      , comment: 'local yum repo packages' }
      - { name: Repo    , url : '/pigsty.repo' , comment: 'local yum repo file' }
      - { name: ISD     , url : '${grafana}/d/isd-overview'    , comment: 'noaa isd data visualization' }
      - { name: Covid   , url : '${grafana}/d/covid-overview'  , comment: 'covid data visualization' }
      - { name: Applog  , url : '${grafana}/d/applog-overview' , comment: 'apple privacy log visualization' }

    docs_enabled: true                            # setup local document under default server?
    pev2_enabled: true                            # setup pev2 explain visualizer under default server?
    pgbadger_enabled: true                        # setup pgbadger under default server?

    # - nameserver - #
    dns_records:                                  # dynamic dns record resolved by dnsmasq
      - 10.10.10.2  pg-meta    # sandbox vip for pg-meta
      - 10.10.10.3  pg-test    # sandbox vip for pg-test
      - 172.21.0.11 meta-1     # sandbox node meta-1
      - 172.21.0.3 node-1      # sandbox node node-1
      - 172.21.0.4 node-2      # sandbox node node-2
      - 172.21.0.16 node-3     # sandbox node node-3
      - 172.21.0.11 pg-meta-1  # sandbox instance pg-meta-1
      - 172.21.0.3 pg-test-1   # sandbox instance node-1
      - 172.21.0.4 pg-test-2   # sandbox instance node-2
      - 172.21.0.16 pg-test-3  # sandbox instance node-3

    # - prometheus - #
    prometheus_data_dir: /data/prometheus/data    # prometheus data dir
    prometheus_options: '--storage.tsdb.retention=30d --enable-feature=promql-negative-offset'
    prometheus_reload: false                      # reload prometheus instead of recreate it
    prometheus_sd_method: static                  # service discovery method: static|consul|etcd
    prometheus_scrape_interval: 5s               # global scrape & evaluation interval
    prometheus_scrape_timeout: 4s                 # scrape timeout
    prometheus_sd_interval: 5s                   # service discovery refresh interval

    # - grafana - #
    grafana_endpoint: http://172.21.0.11:3000     # grafana endpoint url
    grafana_admin_username: admin                 # default grafana admin username
    grafana_admin_password: pigsty                # default grafana admin password
    grafana_database: sqlite3                     # default grafana database type: sqlite3|postgres, sqlite3 by default
    # if postgres is used, url must be specified. The user is pre-defined in pg-meta.pg_users
    grafana_pgurl: postgres://dbuser_grafana:DBUser.Grafana@meta:5436/grafana
    grafana_plugin: install                       # none|install, none will skip plugin installation
    grafana_cache: /www/pigsty/plugins.tgz        # path to grafana plugins cache tarball
    grafana_plugins:                              # plugins that will be downloaded via grafana-cli
      - marcusolsson-csv-datasource
      - marcusolsson-json-datasource
      - marcusolsson-treemap-panel
    grafana_git_plugins:                          # plugins that will be downloaded via git
      - https://github.com/Vonng/vonng-echarts-panel

    # - loki - #                                  # note that loki is not installed by default
    loki_clean: false                             # whether remove existing loki data
    loki_data_dir: /data/loki                     # default loki data dir

    # - jupyter - #
    jupyter_enabled: true                         # setup jupyter lab server?
    jupyter_username: jupyter                     # os user name, special names: default|root (dangerous!)
    jupyter_password: pigstypigsty                # jupyter password (very important if enabled!)

    # - pgweb - #
    pgweb_enabled: true                           # setup pgweb server?
    pgweb_username: pgweb                         # os user name, special names: default|root (dangerous!)


    #------------------------------------------------------------------------------
    # DCS PROVISION
    #------------------------------------------------------------------------------
    service_registry: consul                      # where to register services: none | consul | etcd | both
    dcs_type: consul                              # consul | etcd | both
    dcs_name: pigsty                              # consul dc name | etcd initial cluster token
    dcs_servers:                                  # dcs server dict in name:ip format
      meta-1: 172.21.0.11                         # you could use existing dcs cluster
      # meta-2: 172.21.0.3                       # host which have their IP listed here will be init as server
      # meta-3: 172.21.0.4                       # 3 or 5 dcs nodes are recommend for production environment
    dcs_exists_action: clean                      # abort|skip|clean if dcs server already exists
    dcs_disable_purge: false                      # set to true to disable purge functionality for good (force dcs_exists_action = abort)
    consul_data_dir: /var/lib/consul              # consul data dir (/var/lib/consul by default)
    etcd_data_dir: /var/lib/etcd                  # etcd data dir (/var/lib/consul by default)


    #------------------------------------------------------------------------------
    # POSTGRES INSTALLATION
    #------------------------------------------------------------------------------
    # - dbsu - #
    pg_dbsu: postgres                             # os user for database, postgres by default (unwise to change it)
    pg_dbsu_uid: 26                               # os dbsu uid and gid, 26 for default postgres users and groups
    pg_dbsu_sudo: limit                           # dbsu sudo privilege: none|limit|all|nopass, limit by default
    pg_dbsu_home: /var/lib/pgsql                  # postgresql home directory
    pg_dbsu_ssh_exchange: true                    # exchange postgres dbsu ssh key among same cluster ?

    # - postgres packages - #                     # `${pg_version} will be replaced by actual `pg_version`
    pg_version: 14                                # default postgresql version to be installed
    pgdg_repo: false                              # add pgdg official repo before install (in case of no local repo available)
    pg_add_repo: false                            # add postgres related repo before install (useful if you want a simple install)
    pg_bin_dir: /usr/pgsql/bin                    # postgres binary dir, default is /usr/pgsql/bin, which use /usr/pgsql -> /usr/pgsql-{ver}
    pg_packages:                                  # postgresql related packages. `${pg_version} will be replaced by `pg_version`
      - postgresql${pg_version}*                  # postgresql kernel packages
      - postgis31_${pg_version}*                  # postgis
      - timescaledb-2-postgresql-${pg_version}    # timescaledb
      - citus_${pg_version}                       # citus
      - pgbouncer patroni pg_exporter pgbadger    # 3rd utils
      - patroni patroni-consul patroni-etcd pgbouncer pgbadger pg_activity
      - python3 python3-psycopg2 python36-requests python3-etcd python3-consul
      - python36-urllib3 python36-idna python36-pyOpenSSL python36-cryptography

    pg_extensions:                                # postgresql extensions. `${pg_version} will be replaced by `pg_version`
      - pg_repack_${pg_version} pg_qualstats_${pg_version} pg_stat_kcache_${pg_version} pg_stat_monitor_${pg_version} wal2json_${pg_version}


    #------------------------------------------------------------------------------
    # POSTGRES PROVISION
    #------------------------------------------------------------------------------
    # - identity - #
    # pg_cluster:                                 # [REQUIRED] cluster name (cluster level,  validated during pg_preflight)
    # pg_seq: 0                                   # [REQUIRED] instance seq (instance level, validated during pg_preflight)
    # pg_role: replica                            # [REQUIRED] service role (instance level, validated during pg_preflight)
    # pg_shard:                                   # [OPTIONAL] shard name  (cluster level)
    # pg_sindex:                                  # [OPTIONAl] shard index (cluster level)

    # - identity option -#
    pg_hostname: true                             # overwrite node hostname with pg instance name
    pg_nodename: true                             # overwrite consul nodename with pg instance name

    # - retention - #
    # pg_exists_action, available options: abort|clean|skip
    #  - abort: abort entire play's execution (default)
    #  - clean: remove existing cluster (dangerous)
    #  - skip: end current play for this host
    # pg_exists: false                            # auxiliary flag variable (DO NOT SET THIS)
    pg_exists_action: abort                       # what to do when found running postgres instance ? (clean are JUST FOR DEMO! do not use this on production)
    pg_disable_purge: false                       # set to true to disable pg purge functionality for good (force pg_exists_action = abort)

    # - storage - #
    pg_data: /pg/data                             # postgres data directory (soft link)
    pg_fs_main: /data                             # primary data disk mount point   /pg   -> {{ pg_fs_main }}/postgres/{{ pg_instance }}
    pg_fs_bkup: /data/backups                     # backup disk mount point         /pg/* -> {{ pg_fs_bkup }}/postgres/{{ pg_instance }}/*
    pg_dummy_filesize: 1GiB                      # /pg/dummy hold some disk space for emergency use

    # - connection - #
    pg_listen: '0.0.0.0'                          # postgres listen address, '0.0.0.0' (all ipv4 addr) by default
    pg_port: 5432                                 # postgres port, 5432 by default
    pg_localhost: /var/run/postgresql             # localhost unix socket dir for connection
    # pg_upstream:                                # [OPTIONAL] specify replication upstream, instance level
    # Set on primary instance will transform this cluster into a standby cluster
    # - patroni - #
    # patroni_mode, available options: default|pause|remove
    #   - default: default ha mode
    #   - pause:   into maintenance mode
    #   - remove:  remove patroni after bootstrap
    patroni_mode: default                         # pause|default|remove
    pg_namespace: /pg                             # top level key namespace in dcs
    patroni_port: 8008                            # default patroni port
    patroni_watchdog_mode: automatic              # watchdog mode: off|automatic|required

    pg_conf: tiny.yml                             # pgsql template:  {oltp|olap|crit|tiny}.yml , use tiny for sandbox
    # use oltp|olap|crit for production, or fork your own templates (in ansible templates dir)
    # extension shared libraries to be added
    pg_shared_libraries: 'timescaledb, pg_stat_statements, auto_explain'

    # - flags - #
    pg_backup: false                              # store base backup on this node          (instance level, TBD)
    pg_delay: 0                                   # apply delay for offline|delayed replica (instance level, TBD)

    # - localization - #
    pg_encoding: UTF8                             # database cluster encoding, UTF8 by default
    pg_locale: C                                  # database cluster local, C by default
    pg_lc_collate: C                              # database cluster collate, C by default
    pg_lc_ctype: en_US.UTF8                       # database character type, en_US.UTF8 by default (for i18n full-text search)

    # - pgbouncer - #
    pgbouncer_port: 6432                          # pgbouncer port, 6432 by default
    pgbouncer_poolmode: transaction               # pooling mode: session|transaction|statement, transaction pooling by default
    pgbouncer_max_db_conn: 100                    # max connection to single database, DO NOT set this larger than postgres max conn or db connlimit


    #------------------------------------------------------------------------------
    # POSTGRES TEMPLATE
    #------------------------------------------------------------------------------
    # - template - #
    pg_init: pg-init                              # init script for cluster template

    # - system roles - #
    pg_replication_username: replicator           # system replication user
    pg_replication_password: DBUser.ReplicatoR    # system replication password
    pg_monitor_username: dbuser_monitor           # system monitor user
    pg_monitor_password: DBUser.MonitorXXX        # system monitor password
    pg_admin_username: dbuser_dba                 # system admin user
    pg_admin_password: DBUser.DBAxxx              # system admin password

    # - default roles - #
    pg_default_roles:
      # default roles
      - { name: dbrole_readonly  , login: false , comment: role for global read-only access  }                            # production read-only role
      - { name: dbrole_readwrite , login: false , roles: [dbrole_readonly], comment: role for global read-write access }  # production read-write role
      - { name: dbrole_offline , login: false , comment: role for restricted read-only access (offline instance) }        # restricted-read-only role
      - { name: dbrole_admin , login: false , roles: [pg_monitor, dbrole_readwrite] , comment: role for object creation }  # production DDL change role

      # default users
      - { name: postgres , superuser: true , comment: system superuser }                             # system dbsu, name is designated by `pg_dbsu`
      - { name: dbuser_dba , superuser: true , roles: [dbrole_admin] , comment: system admin user }  # admin dbsu, name is designated by `pg_admin_username`
      - { name: replicator , replication: true , bypassrls: true , roles: [pg_monitor, dbrole_readonly] , comment: system replicator }                   # replicator
      - { name: dbuser_monitor , roles: [pg_monitor, dbrole_readonly] , comment: system monitor user , parameters: {log_min_duration_statement: 1000 } } # monitor user
      - { name: dbuser_stats , password: DBUser.Stats , roles: [dbrole_offline] , comment: business offline user for offline queries and ETL }           # ETL user

    # - privileges - #
    # object created by dbsu and admin will have their privileges properly set
    pg_default_privileges:
      - GRANT USAGE                         ON SCHEMAS   TO dbrole_readonly
      - GRANT SELECT                        ON TABLES    TO dbrole_readonly
      - GRANT SELECT                        ON SEQUENCES TO dbrole_readonly
      - GRANT EXECUTE                       ON FUNCTIONS TO dbrole_readonly
      - GRANT USAGE                         ON SCHEMAS   TO dbrole_offline
      - GRANT SELECT                        ON TABLES    TO dbrole_offline
      - GRANT SELECT                        ON SEQUENCES TO dbrole_offline
      - GRANT EXECUTE                       ON FUNCTIONS TO dbrole_offline
      - GRANT INSERT, UPDATE, DELETE        ON TABLES    TO dbrole_readwrite
      - GRANT USAGE, UPDATE                 ON SEQUENCES TO dbrole_readwrite
      - GRANT TRUNCATE, REFERENCES, TRIGGER ON TABLES    TO dbrole_admin
      - GRANT CREATE                        ON SCHEMAS   TO dbrole_admin

    # - schemas - #
    pg_default_schemas: [ monitor ]               # default schemas to be created

    # - extension - #
    pg_default_extensions:                        # default extensions to be created
      - { name: 'pg_stat_statements', schema: 'monitor' }
      - { name: 'pgstattuple',        schema: 'monitor' }
      - { name: 'pg_qualstats',       schema: 'monitor' }
      - { name: 'pg_buffercache',     schema: 'monitor' }
      - { name: 'pageinspect',        schema: 'monitor' }
      - { name: 'pg_prewarm',         schema: 'monitor' }
      - { name: 'pg_visibility',      schema: 'monitor' }
      - { name: 'pg_freespacemap',    schema: 'monitor' }
      - { name: 'pg_repack',          schema: 'monitor' }
      - name: postgres_fdw
      - name: file_fdw
      - name: btree_gist
      - name: btree_gin
      - name: pg_trgm
      - name: intagg
      - name: intarray

    # - hba - #
    pg_offline_query: false                       # set to true to enable offline query on this instance (instance level)
    pg_reload: true                               # reload postgres after hba changes
    pg_hba_rules:                                 # postgres host-based authentication rules
      - title: allow meta node password access
        role: common
        rules:
          - host    all     all                         172.21.0.11/32      md5

      - title: allow intranet admin password access
        role: common
        rules:
          - host    all     +dbrole_admin               10.0.0.0/8          md5
          - host    all     +dbrole_admin               172.16.0.0/12       md5
          - host    all     +dbrole_admin               192.168.0.0/16      md5

      - title: allow intranet password access
        role: common
        rules:
          - host    all             all                 10.0.0.0/8          md5
          - host    all             all                 172.16.0.0/12       md5
          - host    all             all                 192.168.0.0/16      md5

      - title: allow local read/write (local production user via pgbouncer)
        role: common
        rules:
          - local   all     +dbrole_readonly                                md5
          - host    all     +dbrole_readonly           127.0.0.1/32         md5

      - title: allow offline query (ETL,SAGA,Interactive) on offline instance
        role: offline
        rules:
          - host    all     +dbrole_offline               10.0.0.0/8        md5
          - host    all     +dbrole_offline               172.16.0.0/12     md5
          - host    all     +dbrole_offline               192.168.0.0/16    md5

    pg_hba_rules_extra: []                        # extra hba rules (overwrite by cluster/instance level config)

    pgbouncer_hba_rules:                          # pgbouncer host-based authentication rules
      - title: local password access
        role: common
        rules:
          - local  all          all                                     md5
          - host   all          all                     127.0.0.1/32    md5

      - title: intranet password access
        role: common
        rules:
          - host   all          all                     10.0.0.0/8      md5
          - host   all          all                     172.16.0.0/12   md5
          - host   all          all                     192.168.0.0/16  md5

    pgbouncer_hba_rules_extra: []                 # extra pgbouncer hba rules (overwrite by cluster/instance level config)
    # pg_users: []                                # business users
    # pg_databases: []                            # business databases

    #------------------------------------------------------------------------------
    # MONITOR PROVISION
    #------------------------------------------------------------------------------
    # - install - #
    exporter_install: none                        # none|yum|binary, none by default
    exporter_repo_url: ''                         # if set, repo will be added to /etc/yum.repos.d/ before yum installation

    # - collect - #
    exporter_metrics_path: /metrics               # default metric path for pg related exporter

    # - node exporter - #
    node_exporter_enabled: true                   # setup node_exporter on instance
    node_exporter_port: 9100                      # default port for node exporter
    node_exporter_options: '--no-collector.softnet --no-collector.nvme --collector.ntp --collector.tcpstat --collector.processes'

    # - pg exporter - #
    pg_exporter_config: pg_exporter.yml           # default config files for pg_exporter
    pg_exporter_enabled: true                     # setup pg_exporter on instance
    pg_exporter_port: 9630                        # default port for pg exporter
    pg_exporter_url: ''                           # optional, if not set, generate from reference parameters
    pg_exporter_auto_discovery: true              # optional, discovery available database on target instance ?
    pg_exporter_exclude_database: 'template0,template1,postgres' # optional, comma separated list of database that WILL NOT be monitored when auto-discovery enabled
    pg_exporter_include_database: ''             # optional, comma separated list of database that WILL BE monitored when auto-discovery enabled, empty string will disable include mode
    pg_exporter_options: '--log.level=info --log.format="logger:syslog?appname=pg_exporter&local=7"'

    # - pgbouncer exporter - #
    pgbouncer_exporter_enabled: true              # setup pgbouncer_exporter on instance (if you don't have pgbouncer, disable it)
    pgbouncer_exporter_port: 9631                 # default port for pgbouncer exporter
    pgbouncer_exporter_url: ''                    # optional, if not set, generate from reference parameters
    pgbouncer_exporter_options: '--log.level=info --log.format="logger:syslog?appname=pgbouncer_exporter&local=7"'

    # - promtail - #                              # promtail is a beta feature which requires manual deployment
    promtail_enabled: true                        # enable promtail logging collector?
    promtail_clean: false                         # remove promtail status file? false by default
    promtail_port: 9080                           # default listen address for promtail
    promtail_status_file: /tmp/promtail-status.yml
    promtail_send_url: http://172.21.0.11:3100/loki/api/v1/push  # loki url to receive logs

    #------------------------------------------------------------------------------
    # SERVICE PROVISION
    #------------------------------------------------------------------------------
    pg_weight: 100              # default load balance weight (instance level)

    # - service - #
    pg_services:               # how to expose postgres service in cluster?
      # primary service will route {ip|name}:5433 to primary pgbouncer (5433->6432 rw)
      - name: primary           # service name {{ pg_cluster }}-primary
        src_ip: "*"
        src_port: 5433
        dst_port: pgbouncer     # 5433 route to pgbouncer
        check_url: /primary     # primary health check, success when instance is primary
        selector: "[]"          # select all instance as primary service candidate

      # replica service will route {ip|name}:5434 to replica pgbouncer (5434->6432 ro)
      - name: replica           # service name {{ pg_cluster }}-replica
        src_ip: "*"
        src_port: 5434
        dst_port: pgbouncer
        check_url: /read-only   # read-only health check. (including primary)
        selector: "[]"          # select all instance as replica service candidate
        selector_backup: "[? pg_role == `primary`]"   # primary are used as backup server in replica service

      # default service will route {ip|name}:5436 to primary postgres (5436->5432 primary)
      - name: default           # service's actual name is {{ pg_cluster }}-default
        src_ip: "*"             # service bind ip address, * for all, vip for cluster virtual ip address
        src_port: 5436          # bind port, mandatory
        dst_port: postgres      # target port: postgres|pgbouncer|port_number , pgbouncer(6432) by default
        check_method: http      # health check method: only http is available for now
        check_port: patroni     # health check port:  patroni|pg_exporter|port_number , patroni by default
        check_url: /primary     # health check url path, / as default
        check_code: 200         # health check http code, 200 as default
        selector: "[]"          # instance selector
        haproxy:                # haproxy specific fields
          maxconn: 3000         # default front-end connection
          balance: roundrobin   # load balance algorithm (roundrobin by default)
          default_server_options: 'inter 3s fastinter 1s downinter 5s rise 3 fall 3 on-marked-down shutdown-sessions slowstart 30s maxconn 3000 maxqueue 128 weight 100'

      # offline service will route {ip|name}:5438 to offline postgres (5438->5432 offline)
      - name: offline           # service name {{ pg_cluster }}-offline
        src_ip: "*"
        src_port: 5438
        dst_port: postgres
        check_url: /replica     # offline MUST be a replica
        selector: "[? pg_role == `offline` || pg_offline_query ]"         # instances with pg_role == 'offline' or instance marked with 'pg_offline_query == true'
        selector_backup: "[? pg_role == `replica` && !pg_offline_query]"  # replica are used as backup server in offline service

    pg_services_extra: []        # extra services to be added

    # - haproxy - #
    haproxy_enabled: true                         # enable haproxy among every cluster members
    haproxy_reload: true                          # reload haproxy after config
    haproxy_admin_auth_enabled: false             # enable authentication for haproxy admin?
    haproxy_admin_username: admin                 # default haproxy admin username
    haproxy_admin_password: pigstypigsty          # default haproxy admin password
    haproxy_exporter_port: 9101                   # default admin/exporter port
    haproxy_client_timeout: 24h                   # client side connection timeout
    haproxy_server_timeout: 24h                   # server side connection timeout

    # - vip - #
    vip_mode: none                                # none | l2 | l4
    vip_reload: true                              # whether reload service after config
    # vip_address: 127.0.0.1                      # virtual ip address ip (l2 or l4)
    # vip_cidrmask: 24                            # virtual ip address cidr mask (l2 only)
    # vip_interface: eth0                         # virtual ip network interface (l2 only)

    # - dns - #                                   # NOT IMPLEMENTED
    # dns_mode: vip                               # vip|all|selector: how to resolve cluster DNS?
    # dns_selector: '[]'                          # if dns_mode == vip, filter instances been resolved

...

