---
######################################################################
# File      :   pigsty-dcs3.yml
# Desc      :   Pigsty 3 meta x 3 node config
# Link      :   https://pigsty.cc/#/v-config
# Ctime     :   2020-05-22
# Mtime     :   2022-03-06
# Copyright (C) 2018-2022 Ruohang Feng (rh@vonng.com)
######################################################################

######################################################################
#                        Sandbox (1-node)                            #
#====================================================================#
# admin user : vagrant  (nopass ssh & sudo already set)              #
# 1.  meta-1   :    10.10.10.10     (2 Core | 4GB)    pg-meta-1      #
# 2.  meta-2   :    10.10.10.11     (1 Core | 1GB)    pg-meta-2      #
# 3.  meta-3   :    10.10.10.12     (1 Core | 1GB)    pg-meta-3      #
# (replace these ip if your 4-node env have different ip addr)       #
# VIP:                                                               #
#     pg-meta --->  10.10.10.2 ---> 10.10.10.10                      #
######################################################################
# bootstrap with special playbook demo.yml instead of meta.yml & other
######################################################################


all: # top-level namespace

  children:

    #----------------------------------#
    # meta nodes (admin controller)    #
    #----------------------------------#
    meta:                                  # special group 'meta' marks admin nodes
      vars:                                # with variable 'meta_node = true'
        meta_node: true                    # and set their ansible_group_priority to 99
        ansible_group_priority: 99         # which will overwrite other group definition
      hosts:                               # add meta nodes here
        10.10.10.10: { }
        10.10.10.11: { repo_enabled: false } # disable repo on secondary meta nodes
        10.10.10.12: { repo_enabled: false }

    #----------------------------------#
    # cluster: pg-meta (on meta node)  #
    #----------------------------------#
    pg-meta:
      hosts:
        10.10.10.10: { pg_seq: 1, pg_role: primary }
        10.10.10.11: { pg_seq: 2, pg_role: replica }
        10.10.10.12: { pg_seq: 3, pg_role: replica , pg_offline_query: true }
      vars:
        pg_hostname: true       #  overwrite nodename with pg hostname
        pg_cluster: pg-meta
        pg_users:
          - { name: dbuser_meta , password: DBUser.Meta   , pgbouncer: true , roles: [ dbrole_admin ] , comment: pigsty admin user }
          - { name: dbuser_view , password: DBUser.Viewer , pgbouncer: true , roles: [ dbrole_readonly ] , comment: read-only viewer for meta database }
        pg_databases:
          - name: meta
            baseline: cmdb.sql
            comment: pigsty meta database
            connlimit: -1
            schemas: [ pigsty ]
            extensions:
              - { name: adminpack, schema: pg_catalog }
              - { name: postgis, schema: public }
              - { name: timescaledb }

        vip_mode: none                        # set to 'l2' to enable l2 vip for pg-meta
        # vip_address: 10.10.10.2             # virtual ip address that binds to primary instance of cluster pg-meta
        # vip_cidrmask: 8                     # cidr network mask length
        # vip_interface: eth1                 # interface to add virtual ip


  #==================================================================#
  #                           Globals                                #
  #==================================================================#
  vars:

    #------------------------------------------------------------------------------
    # CONNECTION PARAMETERS
    #------------------------------------------------------------------------------
    # this section defines connection parameters (How to perform ssh sudo on nodes)

    # ansible_user: vagrant                       # admin user with ssh access and sudo privilege
    # ansible_password: <remote ssh pass>         # admin user's ssh password (sshpass required, not recommended)
    # ansible_become_pass: <remote sudo password> # admin user's sudo password (security breach, not recommended)

    proxy_env:                                    # global proxy env when downloading packages
      no_proxy: "localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,*.pigsty,*.aliyun.com,mirrors.*,*.myqcloud.com"
      # http_proxy:  # set your proxy here: e.g http://user:pass@proxy.xxx.com
      # https_proxy: # set your proxy here: e.g http://user:pass@proxy.xxx.com
      # all_proxy:   # set your proxy here: e.g http://user:pass@proxy.xxx.com



    #------------------------------------------------------------------------------
    # REPO PROVISION
    #------------------------------------------------------------------------------
    # this section describes pigsty local yum repo

    # - repo basic - #
    repo_enabled: true                            # build local yum repo on meta nodes?
    repo_name: pigsty                             # local repo name (do not change)
    repo_address: pigsty                          # repo external address (ip:port or url)
    repo_port: 80                                 # listen address, must same as repo_address
    repo_home: /www                               # default repo dir location
    repo_rebuild: false                           # force re-download packages
    repo_remove: true                             # remove existing repos

    # - where to download - #
    repo_upstreams:
      - name: base
        description: CentOS-$releasever - Base
        gpgcheck: no
        baseurl:
          - https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/os/$basearch/ # tuna
          - http://mirrors.aliyun.com/centos/$releasever/os/$basearch/
          - http://mirrors.aliyuncs.com/centos/$releasever/os/$basearch/
          - http://mirrors.cloud.aliyuncs.com/centos/$releasever/os/$basearch/    # aliyun
          - http://mirror.centos.org/centos/$releasever/os/$basearch/             # official

      - name: updates
        description: CentOS-$releasever - Updates
        gpgcheck: no
        baseurl:
          - https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/updates/$basearch/ # tuna
          - http://mirrors.aliyun.com/centos/$releasever/updates/$basearch/
          - http://mirrors.aliyuncs.com/centos/$releasever/updates/$basearch/
          - http://mirrors.cloud.aliyuncs.com/centos/$releasever/updates/$basearch/    # aliyun
          - http://mirror.centos.org/centos/$releasever/updates/$basearch/             # official

      - name: extras
        description: CentOS-$releasever - Extras
        baseurl:
          - https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/extras/$basearch/ # tuna
          - http://mirrors.aliyun.com/centos/$releasever/extras/$basearch/
          - http://mirrors.aliyuncs.com/centos/$releasever/extras/$basearch/
          - http://mirrors.cloud.aliyuncs.com/centos/$releasever/extras/$basearch/    # aliyun
          - http://mirror.centos.org/centos/$releasever/extras/$basearch/             # official
        gpgcheck: no

      - name: epel
        description: CentOS $releasever - epel
        gpgcheck: no
        baseurl:
          - https://mirrors.tuna.tsinghua.edu.cn/epel/$releasever/$basearch   # tuna
          - http://mirrors.aliyun.com/epel/$releasever/$basearch              # aliyun
          - http://download.fedoraproject.org/pub/epel/$releasever/$basearch  # official

      - name: grafana
        description: Grafana
        enabled: yes
        gpgcheck: no
        baseurl:
          - https://mirrors.tuna.tsinghua.edu.cn/grafana/yum/rpm    # tuna mirror
          - https://packages.grafana.com/oss/rpm                    # official

      - name: prometheus
        description: Prometheus and exporters
        gpgcheck: no
        baseurl: https://packagecloud.io/prometheus-rpm/release/el/$releasever/$basearch # no other mirrors, quite slow

      - name: pgdg-common
        description: PostgreSQL common RPMs for RHEL/CentOS $releasever - $basearch
        gpgcheck: no
        baseurl:
          - http://mirrors.tuna.tsinghua.edu.cn/postgresql/repos/yum/common/redhat/rhel-$releasever-$basearch  # tuna
          - https://download.postgresql.org/pub/repos/yum/common/redhat/rhel-$releasever-$basearch             # official

      - name: pgdg13
        description: PostgreSQL 13 for RHEL/CentOS $releasever - $basearch
        gpgcheck: no
        baseurl:
          - https://mirrors.tuna.tsinghua.edu.cn/postgresql/repos/yum/13/redhat/rhel-$releasever-$basearch    # tuna
          - https://download.postgresql.org/pub/repos/yum/13/redhat/rhel-$releasever-$basearch                # official

      - name: pgdg14
        description: PostgreSQL 14 for RHEL/CentOS $releasever - $basearch
        gpgcheck: no
        baseurl:
          - https://mirrors.tuna.tsinghua.edu.cn/postgresql/repos/yum/14/redhat/rhel-$releasever-$basearch    # tuna
          - https://download.postgresql.org/pub/repos/yum/14/redhat/rhel-$releasever-$basearch                # official

      - name: timescaledb
        description: TimescaleDB for RHEL/CentOS $releasever - $basearch
        gpgcheck: no
        baseurl:
          - https://packagecloud.io/timescale/timescaledb/el/7/$basearch

      - name: centos-sclo
        description: CentOS-$releasever - SCLo
        gpgcheck: no
        baseurl: # mirrorlist: http://mirrorlist.centos.org?arch=$basearch&release=$releasever&repo=sclo-sclo
          - http://mirrors.aliyun.com/centos/$releasever/sclo/$basearch/sclo/
          - http://repo.virtualhosting.hk/centos/$releasever/sclo/$basearch/sclo/

      - name: centos-sclo-rh
        description: CentOS-$releasever - SCLo rh
        gpgcheck: no
        baseurl: # mirrorlist: http://mirrorlist.centos.org?arch=$basearch&release=7&repo=sclo-rh
          - http://mirrors.aliyun.com/centos/$releasever/sclo/$basearch/rh/
          - http://repo.virtualhosting.hk/centos/$releasever/sclo/$basearch/rh/

      - name: nginx
        description: Nginx Official Yum Repo
        skip_if_unavailable: true
        gpgcheck: no
        baseurl: http://nginx.org/packages/centos/$releasever/$basearch/

      # for latest consul & kubernetes
      - name: harbottle
        description: Copr repo for main owned by harbottle
        skip_if_unavailable: true
        gpgcheck: no
        baseurl: https://download.copr.fedorainfracloud.org/results/harbottle/main/epel-$releasever-$basearch/


    # - what to download - #
    repo_packages:
      # repo bootstrap packages
      - epel-release nginx wget yum-utils yum createrepo sshpass zip unzip                    # bootstrap packages

      # node basic packages
      - ntp chrony uuid lz4 nc pv jq vim-enhanced make patch bash lsof wget git tuned perf ftp # basic system util
      - readline zlib openssl libyaml libxml2 libxslt libevent perl perl-devel perl-ExtUtils*  # basic pg dependency
      - numactl grubby sysstat dstat iotop bind-utils net-tools tcpdump socat ipvsadm telnet   # system utils
      - ca-certificates openssh-clients ed mlocate lrzsz parted bzip2 krb5-devel rsync apr apr-util

      # dcs & monitor packages
      - grafana prometheus2 pushgateway alertmanager                                          # monitor and ui
      - node_exporter postgres_exporter nginx_exporter blackbox_exporter redis_exporter       # exporter
      - consul consul_exporter consul-template etcd                                           # dcs

      # python3 dependencies
      - ansible python python-pip python-psycopg2 audit                                       # ansible & python
      - python3 python3-psycopg2 python36-requests python3-etcd python3-consul                # python3
      - python36-urllib3 python36-idna python36-pyOpenSSL python36-cryptography               # patroni extra deps

      # proxy and load balancer
      - keepalived dnsmasq                                                                    # proxy and dns

      # postgres common Packages
      - patroni patroni-consul patroni-etcd pgbouncer pg_cli pgbadger pg_activity             # major components
      - pgcenter boxinfo check_postgres emaj pgbconsole pg_bloat_check pgquarrel              # other common utils
      - barman barman-cli pgloader pgFormatter pitrery pspg pgxnclient PyGreSQL pgadmin4 tail_n_mail

      # postgres 13 packages
      - postgresql13*                                                                          # postgresql 13 kernel
      - postgresql13* postgis31_13* timescaledb-2-postgresql-13 citus_13                       # postgresql 13 extensions
      - pg_repack13 pg_squeeze13 pg_qualstats13 pg_stat_kcache13 system_stats_13 bgw_replstatus13 # stats extensions
      - plr13 plsh13 plpgsql_check_13 plproxy13 plr13 plsh13 plpgsql_check_13 pldebugger13     # PL extensions
      - hdfs_fdw_13 mongo_fdw13 mysql_fdw_13 ogr_fdw13 redis_fdw_13 pgbouncer_fdw13            # FDW extensions
      - wal2json13 count_distinct13 ddlx_13 geoip13 orafce13                                   # MISC extensions
      - rum_13 hypopg_13 ip4r13 jsquery_13 logerrors_13 periods_13 pg_auto_failover_13 pg_catcheck13
      - pg_fkpart13 pg_jobmon13 pg_partman13 pg_prioritize_13 pg_track_settings13 pgaudit15_13
      - pgcryptokey13 pgexportdoc13 pgimportdoc13 pgmemcache-13 pgmp13 pgq-13
      - pguint13 pguri13 prefix13  safeupdate_13 semver13  table_version13 tdigest13

      # postgres 14 packages
      - postgresql14* postgis31_14* postgis32_14* citus_14* pg_repack_14 pglogical_14* timescaledb-2-postgresql-14
      - pg_repack_14 wal2json_14 pg_qualstats_14 pg_stat_kcache_14 pg_stat_monitor_14
      - pg_top_14 pg_statement_rollback_14 system_stats_14 pg_track_settings_14 pg_wait_sampling_14
      - plproxy_14 plr_14 plsh_14 pldebugger_14 plpgsql_check_14 pgmemcache_14
      - mysql_fdw_14 ogr_fdw_14 tds_fdw_14 sqlite_fdw_14 firebird_fdw_14 hdfs_fdw_14 mongo_fdw_14 osm_fdw_14 pgbouncer_fdw_14
      - hypopg_14 geoip_14 rum_14 hll_14 ip4r_14 prefix_14 pguri_14 tdigest_14 topn_14 periods_14
      - bgw_replstatus_14 count_distinct_14 credcheck_14 ddlx_14 extra_window_functions_14 logerrors_14 mysqlcompat_14 orafce_14
      - repmgr_14 pg_auth_mon_14 pg_auto_failover_14 pg_background_14 pg_bulkload_14 pg_catcheck_14 pg_comparator_14
      - pg_cron_14 pg_fkpart_14 pg_jobmon_14 pg_partman_14 pg_permissions_14 pg_prioritize_14 pgagent_14
      - pgaudit16_14 pgauditlogtofile_14 pgcryptokey_14 pgexportdoc_14 pgfincore_14 pgimportdoc_14 powa_14 pgmp_14 pgq_14
      - pgquarrel-0.7.0-1 pgsql_tweaks_14 pgtap_14 pgtt_14 postgresql-unit_14 postgresql_anonymizer_14 postgresql_faker_14
      - safeupdate_14 semver_14 set_user_14 sslutils_14 table_version_14 # pgrouting_14 osm2pgrouting_14

      # build & devel packages (optional)
      - gcc gcc-c++ clang coreutils diffutils rpm-build rpm-devel rpmlint rpmdevtools bison flex
      - readline-devel zlib-devel uuid-devel libuuid-devel libxml2-devel libxslt-devel openssl-devel libicu-devel

    repo_url_packages:
      # mandatory rpm
      - https://github.com/cybertec-postgresql/vip-manager/releases/download/v1.0.1/vip-manager_1.0.1-1_amd64.rpm
      - https://github.com/Vonng/pg_exporter/releases/download/v0.4.1/pg_exporter-0.4.1-1.el7.x86_64.rpm
      - https://github.com/Vonng/pigsty-pkg/releases/download/init/haproxy-2.5.1-1.el7.x86_64.rpm
      - https://github.com/Vonng/pigsty-pkg/releases/download/init/haproxy-debuginfo-2.5.1-1.el7.x86_64.rpm

      # extra database support
      - https://rpmfind.net/linux/remi/enterprise/7/remi/x86_64/redis-6.2.6-1.el7.remi.x86_64.rpm
      - https://github.com/greenplum-db/gpdb/releases/download/6.19.1/open-source-greenplum-db-6.19.1-rhel7-x86_64.rpm
      - http://guichaz.free.fr/polysh/files/polysh-0.4-1.noarch.rpm
      - https://github.com/dalibo/pev2/releases/download/v0.24.0/pev2.tar.gz
      - https://github.com/sosedoff/pgweb/releases/download/v0.11.10/pgweb_linux_amd64.zip
      - https://github.com/PostgREST/postgrest/releases/download/v9.0.0/postgrest-v9.0.0-linux-static-x64.tar.xz

      # binary install mode
      - https://github.com/Vonng/pg_exporter/releases/download/v0.4.0/pg_exporter_v0.4.0_linux-amd64.tar.gz
      - https://github.com/prometheus/node_exporter/releases/download/v1.3.1/node_exporter-1.3.1.linux-amd64.tar.gz
      - https://github.com/grafana/loki/releases/download/v2.4.2/loki-linux-amd64.zip
      - https://github.com/grafana/loki/releases/download/v2.4.2/promtail-linux-amd64.zip
      - https://github.com/grafana/loki/releases/download/v2.4.2/logcli-linux-amd64.zip
      - https://github.com/grafana/loki/releases/download/v2.4.2/loki-canary-linux-amd64.zip


    #------------------------------------------------------------------------------
    # NODE PROVISION
    #------------------------------------------------------------------------------
    # this section defines how to provision nodes

    # - node flag - #
    meta_node: false                              # node with meta_node will be marked as admin nod

    # - node identity - #
    # nodename:                                   # if provided, node's hostname will be overwritten
    node_cluster: nodes                           # node's cluster label will be set to this (nodes by default)
    node_name_exchange: false                     # exchange hostname among play hosts ?

    # - node dns - #
    node_dns_hosts:                               # static dns records in /etc/hosts
      - 10.10.10.10 meta pigsty c.pigsty g.pigsty p.pigsty a.pigsty cli.pigsty lab.pigsty
    node_dns_hosts_extra: []                      # extra static dns records in /etc/hosts

    node_dns_server: add                          # add (default) | none (skip) | overwrite (remove old settings)
    node_dns_servers:                             # dynamic nameserver in /etc/resolv.conf
      - 10.10.10.10
    node_dns_options:                             # dns resolv options
      - options single-request-reopen timeout:1 rotate
      - domain service.consul

    # - node repo - #
    node_repo_method: local                       # none|local|public (use local repo for production env)
    node_repo_remove: true                        # whether remove existing repo
    node_local_repo_url:                          # local repo url (if method=local, make sure firewall is configured or disabled)
      - http://pigsty/pigsty.repo

    # - node packages - #
    node_packages:                                # common packages for all nodes
      - wget,yum-utils,sshpass,ntp,chrony,tuned,uuid,lz4,vim-minimal,make,patch,bash,lsof,wget,unzip,git,ftp
      - numactl,grubby,sysstat,dstat,iotop,bind-utils,net-tools,tcpdump,socat,ipvsadm,telnet,tuned,pv,jq,perf
      - readline,zlib,openssl,openssl-libs,python3,python36-requests,node_exporter,redis_exporter,consul,etcd
    node_extra_packages: [ ]                      # extra packages for all nodes
    node_meta_packages:                           # packages for meta nodes only
      - grafana,prometheus2,alertmanager,nginx_exporter,blackbox_exporter,pushgateway,redis,postgresql14
      - nginx,ansible,pgbadger,python-psycopg2,dnsmasq,polysh
      - clang,coreutils,diffutils,rpm-build,rpm-devel,rpmlint,rpmdevtools,bison,flex # gcc,gcc-c++
      - readline-devel,zlib-devel,uuid-devel,libuuid-devel,libxml2-devel,libxslt-devel,openssl-devel,libicu-devel
    node_meta_pip_install: 'jupyterlab'           # pip packages installed on meta

    # - node features - #
    node_disable_numa: false                      # disable numa, reboot required
    node_disable_swap: false                      # disable swap, use with caution
    node_disable_firewall: true                   # disable firewall
    node_disable_selinux: true                    # disable selinux
    node_static_network: true                     # keep dns resolver settings after reboot
    node_disk_prefetch: false                     # setup disk prefetch on HDD to increase performance

    # - node kernel modules - #
    node_kernel_modules: [softdog, br_netfilter, ip_vs, ip_vs_rr, ip_vs_rr, ip_vs_wrr, ip_vs_sh]

    # - node tuned - #
    node_tune: tiny                               # install and activate tuned profile: none|oltp|olap|crit|tiny
    node_sysctl_params: { }                       # set additional sysctl parameters, k:v format
    # net.bridge.bridge-nf-call-iptables: 1       # example sysctl parameters

    # - node admin - #
    node_admin_setup: true                        # create a default admin user defined by `node_admin_*` ?
    node_admin_uid: 88                            # uid and gid for this admin user
    node_admin_username: dba                      # name of this admin user, dba by default
    node_admin_ssh_exchange: true                 # exchange admin ssh key among each pgsql cluster ?
    node_admin_pk_current: true                   # add current user's ~/.ssh/id_rsa.pub to admin authorized_keys ?
    node_admin_pks:                               # ssh public keys to be added to admin user (REPLACE WITH YOURS!)
      - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAAAgQC7IMAMNavYtWwzAJajKqwdn3ar5BhvcwCnBTxxEkXhGlCO2vfgosSAQMEflfgvkiI5nM1HIFQ8KINlx1XLO7SdL5KdInG5LIJjAFh0pujS4kNCT9a5IGvSq1BrzGqhbEcwWYdju1ZPYBcJm/MG+JD0dYCh8vfrYB/cYMD0SOmNkQ== vagrant@pigsty.com'

    # - node tz - #
    node_timezone: Asia/Hong_Kong                 # default node timezone, empty will not change

    # - node ntp - #
    node_ntp_config: true                         # config ntp service? false will leave it with system default
    node_ntp_service: ntp                         # ntp service provider: ntp|chrony
    node_ntp_servers:                             # default NTP servers
      - pool cn.pool.ntp.org iburst
      - pool pool.ntp.org iburst
      - pool time.pool.aliyun.com iburst
      - server 10.10.10.10 iburst
      - server ntp.tuna.tsinghua.edu.cn iburst


    #------------------------------------------------------------------------------
    # META PROVISION
    #------------------------------------------------------------------------------
    # - ca - #
    ca_method: create                             # create|copy|recreate
    ca_subject: "/CN=root-ca"                     # self-signed CA subject
    ca_homedir: /ca                               # ca cert directory
    ca_cert: ca.crt                               # ca public key/cert
    ca_key: ca.key                                # ca private key

    # - nginx - #
    nginx_upstream:                               # domain names that will be used for accessing pigsty services
      - { name: home,          domain: pigsty,        endpoint: "10.10.10.10:80" }     # default -> index.html (80)
      - { name: grafana,       domain: g.pigsty,      endpoint: "10.10.10.10:3000" }   # pigsty grafana (3000)
      - { name: prometheus,    domain: p.pigsty,      endpoint: "10.10.10.10:9090" }   # pigsty prometheus (9090)
      - { name: alertmanager,  domain: a.pigsty,      endpoint: "10.10.10.10:9093" }   # pigsty alertmanager (9093)
      # some service can only be accessed via domain name due to security reasons (e.g consul, pgweb, jupyter)
      - { name: consul,        domain: c.pigsty,      endpoint: "127.0.0.1:8500" }     # pigsty consul UI (8500) (domain required)
      - { name: pgweb,         domain: cli.pigsty,    endpoint: "127.0.0.1:8081" }     # pgweb console (8081)
      - { name: jupyter,       domain: lab.pigsty,    endpoint: "127.0.0.1:8888" }     # jupyter lab (8888)

    # - app - #
    app_list:                                      # show extra application links on home page
      - { name: Pev2    , url : '/pev2'        , comment: 'postgres explain visualizer 2' }
      - { name: Logs    , url : '/logs'        , comment: 'realtime pgbadger log sample' }
      - { name: Report  , url : '/report'      , comment: 'daily log summary report ' }
      - { name: Pkgs    , url : '/pigsty'      , comment: 'local yum repo packages' }
      - { name: Repo    , url : '/pigsty.repo' , comment: 'local yum repo file' }
      - { name: ISD     , url : '${grafana}/d/isd-overview'   , comment: 'noaa isd data visualization' }
      - { name: Covid   , url : '${grafana}/d/covid-overview' , comment: 'covid data visualization' }
      - { name: Applog  , url : '${grafana}/d/applog-overview', comment: 'apple privacy log analysis' }

    docs_enabled: true                            # setup local document under default server?
    pev2_enabled: true                            # setup pev2 explain visualizer under default server?
    pgbadger_enabled: true                        # setup pgbadger under default server?

    # - nameserver - #
    dns_records:                                  # dynamic dns record resolved by dnsmasq
      - 10.10.10.2  pg-meta    # sandbox vip for pg-meta
      - 10.10.10.10 meta-1     # sandbox node meta-1
      - 10.10.10.11 meta-2     # sandbox node node-1
      - 10.10.10.12 meta-3     # sandbox node node-2
      - 10.10.10.10 pg-meta-1  # sandbox instance pg-meta-1
      - 10.10.10.11 pg-meta-2  # sandbox instance node-1
      - 10.10.10.12 pg-meta-3  # sandbox instance node-2

    # - prometheus - #
    prometheus_data_dir: /data/prometheus/data    # prometheus data dir
    prometheus_options: '--storage.tsdb.retention=15d --enable-feature=promql-negative-offset'
    prometheus_reload: false                      # reload prometheus instead of recreate it
    prometheus_sd_method: static                  # service discovery method: static|consul|etcd
    prometheus_scrape_interval: 10s               # global scrape & evaluation interval
    prometheus_scrape_timeout: 8s                 # scrape timeout
    prometheus_sd_interval: 10s                   # service discovery refresh interval

    # - grafana - #
    grafana_endpoint: http://10.10.10.10:3000     # grafana endpoint url
    grafana_admin_username: admin                 # default grafana admin username
    grafana_admin_password: pigsty                # default grafana admin password
    grafana_database: sqlite3                     # default grafana database type: sqlite3|postgres, sqlite3 by default
    # if postgres is used, url must be specified. The user is pre-defined in pg-meta.pg_users
    grafana_pgurl: postgres://dbuser_grafana:DBUser.Grafana@meta:5436/grafana
    grafana_plugin: install                       # none|install, none will skip plugin installation
    grafana_cache: /www/pigsty/plugins.tgz        # path to grafana plugins cache tarball
    grafana_plugins:                              # plugins that will be downloaded via grafana-cli
      - marcusolsson-csv-datasource
      - marcusolsson-json-datasource
      - marcusolsson-treemap-panel
    grafana_git_plugins:                          # plugins that will be downloaded via git
      - https://github.com/Vonng/vonng-echarts-panel

    #------------------------------------------------------------------------------
    # DCS PROVISION
    #------------------------------------------------------------------------------
    service_registry: consul                      # where to register services: none | consul | etcd | both
    dcs_type: consul                              # consul | etcd | both
    dcs_name: pigsty                              # consul dc name | etcd initial cluster token
    dcs_servers:                                  # dcs server dict in name:ip format
      pg-meta-1: 10.10.10.10                      # you could use existing dcs cluster
      pg-meta-2: 10.10.10.11                      # host which have their IP listed here will be init as server
      pg-meta-3: 10.10.10.12                      # 3 or 5 dcs nodes are recommended for production environment
    dcs_exists_action: clean                      # abort|skip|clean if dcs server already exists
    dcs_disable_purge: false                      # set to true to disable purge functionality for good (force dcs_exists_action = abort)
    consul_data_dir: /var/lib/consul              # consul data dir (/var/lib/consul by default)
    etcd_data_dir: /var/lib/etcd                  # etcd data dir (/var/lib/consul by default)


    #------------------------------------------------------------------------------
    # POSTGRES INSTALLATION
    #------------------------------------------------------------------------------
    # - dbsu - #
    pg_dbsu: postgres                             # os user for database, postgres by default (unwise to change it)
    pg_dbsu_uid: 26                               # os dbsu uid and gid, 26 for default postgres users and groups
    pg_dbsu_sudo: limit                           # dbsu sudo privilege: none|limit|all|nopass, limit by default
    pg_dbsu_home: /var/lib/pgsql                  # postgresql home directory
    pg_dbsu_ssh_exchange: true                    # exchange postgres dbsu ssh key among same cluster ?

    # - postgres packages - #                     # `${pg_version} will be replaced by actual `pg_version`
    pg_version: 14                                # default postgresql version to be installed
    pgdg_repo: false                              # add pgdg official repo before install (in case of no local repo available)
    pg_add_repo: false                            # add postgres related repo before install (useful if you want a simple install)
    pg_bin_dir: /usr/pgsql/bin                    # postgres binary dir, default is /usr/pgsql/bin, which use /usr/pgsql -> /usr/pgsql-{ver}
    pg_packages:                                  # postgresql related packages. `${pg_version} will be replaced by `pg_version`
      - postgresql${pg_version}*                  # postgresql kernel packages
      - postgis32_${pg_version}*                  # postgis
      - citus_${pg_version}*                      # citus
      - timescaledb-2-postgresql-${pg_version}    # timescaledb
      - pgbouncer pg_exporter pgbadger pg_activity node_exporter consul haproxy vip-manager
      - patroni patroni-consul patroni-etcd python3 python3-psycopg2 python36-requests python3-etcd
      - python3-consul python36-urllib3 python36-idna python36-pyOpenSSL python36-cryptography

    pg_extensions:                                # postgresql extensions. `${pg_version} will be replaced by `pg_version`
      - pg_repack_${pg_version} pg_qualstats_${pg_version} pg_stat_kcache_${pg_version} pg_stat_monitor_${pg_version} wal2json_${pg_version}


    #------------------------------------------------------------------------------
    # POSTGRES PROVISION
    #------------------------------------------------------------------------------
    # - identity - #
    # pg_cluster:                                 # [REQUIRED] cluster name (cluster level,  validated during pg_preflight)
    # pg_seq: 0                                   # [REQUIRED] instance seq (instance level, validated during pg_preflight)
    # pg_role: replica                            # [REQUIRED] service role (instance level, validated during pg_preflight)
    # pg_shard:                                   # [OPTIONAL] shard name  (cluster level)
    # pg_sindex:                                  # [OPTIONAl] shard index (cluster level)
    pg_hostname: true                             # use postgres instance name as node hostname?

    # - retention - #
    # pg_exists_action, available options: abort|clean|skip
    #  - abort: abort entire play's execution (default)
    #  - clean: remove existing cluster (dangerous)
    #  - skip: end current play for this host
    # pg_exists: false                            # auxiliary flag variable (DO NOT SET THIS)
    pg_exists_action: abort                       # what to do when found running postgres instance ? (clean are JUST FOR DEMO! do not use this on production)
    pg_disable_purge: false                       # set to true to disable pg purge functionality for good (force pg_exists_action = abort)

    # - storage - #
    pg_data: /pg/data                             # postgres data directory (soft link)
    pg_fs_main: /data                             # primary data disk mount point   /pg   -> {{ pg_fs_main }}/postgres/{{ pg_instance }}
    pg_fs_bkup: /data/backups                     # backup disk mount point         /pg/* -> {{ pg_fs_bkup }}/postgres/{{ pg_instance }}/*
    pg_dummy_filesize: 64MiB                      # /pg/dummy hold some disk space for emergency use

    # - connection - #
    pg_listen: '0.0.0.0'                          # postgres listen address, '0.0.0.0' (all ipv4 addr) by default
    pg_port: 5432                                 # postgres port, 5432 by default
    pg_localhost: /var/run/postgresql             # localhost unix socket dir for connection
    # pg_upstream:                                # [OPTIONAL] specify replication upstream, instance level
    # Set on primary instance will transform this cluster into a standby cluster

    # - patroni - #
    patroni_enabled: true                         # if not enabled, no postgres cluster will be created
    # patroni_mode, available options: default|pause|remove
    #   - default: default ha mode
    #   - pause:   into maintenance mode
    #   - remove:  remove patroni after bootstrap
    patroni_mode: default                         # pause|default|remove
    pg_namespace: /pg                             # top level key namespace in dcs
    patroni_port: 8008                            # default patroni port
    patroni_watchdog_mode: automatic              # watchdog mode: off|automatic|required

    pg_conf: tiny.yml                             # pgsql template:  {oltp|olap|crit|tiny}.yml , use tiny for sandbox
    # use oltp|olap|crit for production, or fork your own templates (in ansible templates dir)
    # extension shared libraries to be added
    pg_shared_libraries: 'timescaledb, pg_stat_statements, auto_explain'

    # - flags - #
    pg_backup: false                              # store base backup on this node          (instance level, TBD)
    pg_delay: 0                                   # apply delay for offline|delayed replica (instance level, TBD)

    # - localization - #
    pg_encoding: UTF8                             # database cluster encoding, UTF8 by default
    pg_locale: C                                  # database cluster local, C by default
    pg_lc_collate: C                              # database cluster collate, C by default
    pg_lc_ctype: en_US.UTF8                       # database character type, en_US.UTF8 by default (for i18n full-text search)

    # - pgbouncer - #
    pgbouncer_enabled: true                       # if not enabled, pgbouncer will not be created
    pgbouncer_port: 6432                          # pgbouncer port, 6432 by default
    pgbouncer_poolmode: transaction               # pooling mode: session|transaction|statement, transaction pooling by default
    pgbouncer_max_db_conn: 100                    # max connection to single database, DO NOT set this larger than postgres max conn or db connlimit


    #------------------------------------------------------------------------------
    # POSTGRES TEMPLATE
    #------------------------------------------------------------------------------
    pg_provision: true                            # whether provisioning postgres cluster

    # - template - #
    pg_init: pg-init                              # init script for cluster template

    # - system roles - #
    pg_replication_username: replicator           # system replication user
    pg_replication_password: DBUser.Replicator    # system replication password
    pg_monitor_username: dbuser_monitor           # system monitor user
    pg_monitor_password: DBUser.Monitor           # system monitor password
    pg_admin_username: dbuser_dba                 # system admin user
    pg_admin_password: DBUser.DBA                 # system admin password

    # - default roles - #
    pg_default_roles:
      # default roles
      - { name: dbrole_readonly  , login: false , comment: role for global read-only access  }                            # production read-only role
      - { name: dbrole_readwrite , login: false , roles: [dbrole_readonly], comment: role for global read-write access }  # production read-write role
      - { name: dbrole_offline , login: false , comment: role for restricted read-only access (offline instance) }        # restricted-read-only role
      - { name: dbrole_admin , login: false , roles: [pg_monitor, dbrole_readwrite] , comment: role for object creation }  # production DDL change role

      # default users
      - { name: postgres , superuser: true , comment: system superuser }                             # system dbsu, name is designated by `pg_dbsu`
      - { name: dbuser_dba , superuser: true , roles: [dbrole_admin] , comment: system admin user }  # admin dbsu, name is designated by `pg_admin_username`
      - { name: replicator , replication: true , bypassrls: true , roles: [pg_monitor, dbrole_readonly] , comment: system replicator }                   # replicator
      - { name: dbuser_monitor , roles: [pg_monitor, dbrole_readonly] , comment: system monitor user , parameters: {log_min_duration_statement: 1000 } } # monitor user
      - { name: dbuser_stats , password: DBUser.Stats , roles: [dbrole_offline] , comment: business offline user for offline queries and ETL }           # ETL user

    # - privileges - #
    # object created by dbsu and admin will have their privileges properly set
    pg_default_privileges:
      - GRANT USAGE                         ON SCHEMAS   TO dbrole_readonly
      - GRANT SELECT                        ON TABLES    TO dbrole_readonly
      - GRANT SELECT                        ON SEQUENCES TO dbrole_readonly
      - GRANT EXECUTE                       ON FUNCTIONS TO dbrole_readonly
      - GRANT USAGE                         ON SCHEMAS   TO dbrole_offline
      - GRANT SELECT                        ON TABLES    TO dbrole_offline
      - GRANT SELECT                        ON SEQUENCES TO dbrole_offline
      - GRANT EXECUTE                       ON FUNCTIONS TO dbrole_offline
      - GRANT INSERT, UPDATE, DELETE        ON TABLES    TO dbrole_readwrite
      - GRANT USAGE, UPDATE                 ON SEQUENCES TO dbrole_readwrite
      - GRANT TRUNCATE, REFERENCES, TRIGGER ON TABLES    TO dbrole_admin
      - GRANT CREATE                        ON SCHEMAS   TO dbrole_admin

    # - schemas - #
    pg_default_schemas: [ monitor ]               # default schemas to be created

    # - extension - #
    pg_default_extensions:                        # default extensions to be created
      - { name: 'pg_stat_statements', schema: 'monitor' }
      - { name: 'pgstattuple',        schema: 'monitor' }
      - { name: 'pg_qualstats',       schema: 'monitor' }
      - { name: 'pg_buffercache',     schema: 'monitor' }
      - { name: 'pageinspect',        schema: 'monitor' }
      - { name: 'pg_prewarm',         schema: 'monitor' }
      - { name: 'pg_visibility',      schema: 'monitor' }
      - { name: 'pg_freespacemap',    schema: 'monitor' }
      - { name: 'pg_repack',          schema: 'monitor' }
      - name: postgres_fdw
      - name: file_fdw
      - name: btree_gist
      - name: btree_gin
      - name: pg_trgm
      - name: intagg
      - name: intarray

    # - hba - #
    pg_offline_query: false                       # set to true to enable offline query on this instance (instance level)
    pg_reload: true                               # reload postgres after hba changes
    pg_hba_rules:                                 # postgres host-based authentication rules
      - title: allow meta node password access
        role: common
        rules:
          - host    all     all                         10.10.10.10/32      md5

      - title: allow intranet admin password access
        role: common
        rules:
          - host    all     +dbrole_admin               10.0.0.0/8          md5
          - host    all     +dbrole_admin               172.16.0.0/12       md5
          - host    all     +dbrole_admin               192.168.0.0/16      md5

      - title: allow intranet password access
        role: common
        rules:
          - host    all             all                 10.0.0.0/8          md5
          - host    all             all                 172.16.0.0/12       md5
          - host    all             all                 192.168.0.0/16      md5

      - title: allow local read/write (local production user via pgbouncer)
        role: common
        rules:
          - local   all     +dbrole_readonly                                md5
          - host    all     +dbrole_readonly           127.0.0.1/32         md5

      - title: allow offline query (ETL,SAGA,Interactive) on offline instance
        role: offline
        rules:
          - host    all     +dbrole_offline               10.0.0.0/8        md5
          - host    all     +dbrole_offline               172.16.0.0/12     md5
          - host    all     +dbrole_offline               192.168.0.0/16    md5

    pg_hba_rules_extra: []                        # extra hba rules (overwrite by cluster/instance level config)

    pgbouncer_hba_rules:                          # pgbouncer host-based authentication rules
      - title: local password access
        role: common
        rules:
          - local  all          all                                     md5
          - host   all          all                     127.0.0.1/32    md5

      - title: intranet password access
        role: common
        rules:
          - host   all          all                     10.0.0.0/8      md5
          - host   all          all                     172.16.0.0/12   md5
          - host   all          all                     192.168.0.0/16  md5

    pgbouncer_hba_rules_extra: []                 # extra pgbouncer hba rules (overwrite by cluster/instance level config)
    # pg_users: []                                # business users
    # pg_databases: []                            # business databases

    #------------------------------------------------------------------------------
    # MONITOR PROVISION
    #------------------------------------------------------------------------------
    # - install - #
    exporter_install: none                        # none|yum|binary, none by default
    exporter_repo_url: ''                         # if set, repo will be added to /etc/yum.repos.d/ before yum installation

    # - collect - #
    exporter_metrics_path: /metrics               # default metric path for pg related exporter

    # - node exporter - #
    node_exporter_enabled: true                   # setup node_exporter on instance
    node_exporter_port: 9100                      # default port for node exporter
    node_exporter_options: '--no-collector.softnet --no-collector.nvme --collector.ntp --collector.tcpstat --collector.processes'

    # - pg exporter - #
    pg_exporter_config: pg_exporter.yml           # default config files for pg_exporter
    pg_exporter_enabled: true                     # setup pg_exporter on instance
    pg_exporter_port: 9630                        # default port for pg exporter
    pg_exporter_params: 'host=/var/run/postgresql&sslmode=disable' # url query parameters for pg_exporter
    pg_exporter_url: ''                           # optional, if not set, generate from reference parameters
    pg_exporter_auto_discovery: true              # optional, discovery available database on target instance ?
    pg_exporter_exclude_database: 'template0,template1,postgres' # optional, comma separated list of database that WILL NOT be monitored when auto-discovery enabled
    pg_exporter_include_database: ''             # optional, comma separated list of database that WILL BE monitored when auto-discovery enabled, empty string will disable include mode
    pg_exporter_options: '--log.level=info --log.format="logger:syslog?appname=pg_exporter&local=7"'

    # - pgbouncer exporter - #
    pgbouncer_exporter_enabled: true              # setup pgbouncer_exporter on instance (if you don't have pgbouncer, disable it)
    pgbouncer_exporter_port: 9631                 # default port for pgbouncer exporter
    pgbouncer_exporter_url: ''                    # optional, if not set, generate from reference parameters
    pgbouncer_exporter_options: '--log.level=info --log.format="logger:syslog?appname=pgbouncer_exporter&local=7"'

    # - promtail - #                              # promtail is a beta feature which requires manual deployment
    promtail_enabled: true                        # enable promtail logging collector?
    promtail_clean: false                         # remove promtail status file? false by default
    promtail_port: 9080                           # default listen address for promtail
    promtail_status_file: /tmp/promtail-status.yml
    promtail_send_url: http://10.10.10.10:3100/loki/api/v1/push  # loki url to receive logs

    #------------------------------------------------------------------------------
    # SERVICE PROVISION
    #------------------------------------------------------------------------------
    pg_weight: 100              # default load balance weight (instance level)

    # - service - #
    pg_services:               # how to expose postgres service in cluster?
      # primary service will route {ip|name}:5433 to primary pgbouncer (5433->6432 rw)
      - name: primary           # service name {{ pg_cluster }}-primary
        src_ip: "*"
        src_port: 5433
        dst_port: pgbouncer     # 5433 route to pgbouncer
        check_url: /primary     # primary health check, success when instance is primary
        selector: "[]"          # select all instance as primary service candidate

      # replica service will route {ip|name}:5434 to replica pgbouncer (5434->6432 ro)
      - name: replica           # service name {{ pg_cluster }}-replica
        src_ip: "*"
        src_port: 5434
        dst_port: pgbouncer
        check_url: /read-only   # read-only health check. (including primary)
        selector: "[]"          # select all instance as replica service candidate
        selector_backup: "[? pg_role == `primary` || pg_role == `offline` ]"
        # primary are used as backup server in replica service

      # default service will route {ip|name}:5436 to primary postgres (5436->5432 primary)
      - name: default           # service's actual name is {{ pg_cluster }}-default
        src_ip: "*"             # service bind ip address, * for all, vip for cluster virtual ip address
        src_port: 5436          # bind port, mandatory
        dst_port: postgres      # target port: postgres|pgbouncer|port_number , pgbouncer(6432) by default
        check_method: http      # health check method: only http is available for now
        check_port: patroni     # health check port:  patroni|pg_exporter|port_number , patroni by default
        check_url: /primary     # health check url path, / as default
        check_code: 200         # health check http code, 200 as default
        selector: "[]"          # instance selector
        haproxy:                # haproxy specific fields
          maxconn: 3000         # default front-end connection
          balance: roundrobin   # load balance algorithm (roundrobin by default)
          default_server_options: 'inter 3s fastinter 1s downinter 5s rise 3 fall 3 on-marked-down shutdown-sessions slowstart 30s maxconn 3000 maxqueue 128 weight 100'

      # offline service will route {ip|name}:5438 to offline postgres (5438->5432 offline)
      - name: offline           # service name {{ pg_cluster }}-offline
        src_ip: "*"
        src_port: 5438
        dst_port: postgres
        check_url: /replica     # offline MUST be a replica
        selector: "[? pg_role == `offline` || pg_offline_query ]"         # instances with pg_role == 'offline' or instance marked with 'pg_offline_query == true'
        selector_backup: "[? pg_role == `replica` && !pg_offline_query]"  # replica are used as backup server in offline service

    pg_services_extra: []        # extra services to be added

    # - haproxy - #
    haproxy_enabled: true                         # enable haproxy among every cluster members
    haproxy_reload: true                          # reload haproxy after config
    haproxy_admin_auth_enabled: false             # enable authentication for haproxy admin?
    haproxy_admin_username: admin                 # default haproxy admin username
    haproxy_admin_password: pigsty                # default haproxy admin password
    haproxy_exporter_port: 9101                   # default admin/exporter port
    haproxy_client_timeout: 24h                   # client side connection timeout
    haproxy_server_timeout: 24h                   # server side connection timeout

    # - vip - #
    vip_mode: none                                # none | l2 | l4
    vip_reload: true                              # whether reload service after config
    # vip_address: 127.0.0.1                      # virtual ip address ip (l2 or l4)
    # vip_cidrmask: 24                            # virtual ip address cidr mask (l2 only)
    # vip_interface: eth0                         # virtual ip network interface (l2 only)

    # - dns - #                                   # NOT IMPLEMENTED
    # dns_mode: vip                               # vip|all|selector: how to resolve cluster DNS?
    # dns_selector: '[]'                          # if dns_mode == vip, filter instances been resolved

...