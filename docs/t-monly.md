# Monitor-Only Deployment

> How to integrate Pigsty with external provisioning solutions, using only the monitoring system part of Pigsty.

If you want to use only the **monitoring system** part of Pigsty, for example if you want to use the Pigsty monitoring system to monitor an existing PostgreSQL instance, then you can use the **monitor only deployment** mode.

### Assumptions

Pigsty Monitoring System To work with an external provisioning solution to monitor an existing database cluster, a number of **working assumptions** are required.

- The database is deployed **exclusively** and there is a **one-to-one** relationship with the nodes. Only then can node metrics be meaningfully associated with database metrics.
- Target nodes can be managed by Ansible (NOPASS SSH vs. NOPASS SUDO), which some cloud vendor RDS products do not allow.
- The database needs to create **monitoring users** that can be used to access the monitoring metrics, install the required monitoring schema and extensions, and configure their access control permissions appropriately.

### Procedure

The deployment process of the monitoring-only schema remains largely the same as the standard schema, but omits many steps

- Complete the [**infrastructure initialization**](p-infra.md) section on the **meta-node**, which is consistent with the standard process****.
- Modify the configuration file, in monitoring-only mode, usually only the parameters in the [monitoring system](v-monitor.md) section need to be modified.
- Execute a subset of [`pgsql.yml`](p-pgsql) to simply complete the deployment of the monitoring system.



## Instructions

### Monitoring users

Database clusters created by Pigsty create system users for monitoring during the [database deployment](v-pg-provision) phase. Monitoring-only mode skips these steps, so users need to **create** their own users for monitoring.

You need to manually create the monitoring user (default is `dbuser_monitor`) in the target database cluster, as well as the monitoring-related **schema** and **extensions**. And adjust the [access control](c-auth.md) mechanism of the target database cluster to allow using this user to connect to the database and access monitoring related objects. The reference SQL statement for creating the monitoring object is as follows.


```sql
-- create monitor user
CREATE USER dbuser_monitor;
ALTER ROLE dbuser_monitor PASSWORD 'DBUser.Monitor';
ALTER USER dbuser_monitor CONNECTION LIMIT 16;
GRANT pg_monitor TO dbuser_monitor;
    
-- create monitor schema
CREATE SCHEMA IF NOT EXISTS monitor;
GRANT USAGE ON SCHEMA monitor TO dbuser_monitor;

-- create monitor extension
CREATE EXTENSION IF NOT EXISTS pg_stat_statements WITH SCHEMA monitor;
```

### Connection String

By default, Pigsty tries to generate a connection string between the database and the connection pool using the following rules.

```bash
PG_EXPORTER_URL='postgres://{{ pg_monitor_username }}:{{ pg_monitor_password }}@:{{ pg_port }}/postgres?host={{ pg_localhost }}&sslmode= disable'
PGBOUNCER_EXPORTER_URL='postgres://{{ pg_monitor_username }}:{{ pg_monitor_password }}@:{{ pgbouncer_port }}/pgbouncer?host={{ pg_ localhost }}&sslmode=disable'
```

If the user is using a monitoring role connection string that cannot be generated by this rule, the database connection information to the connection pool can be configured directly using the following parameters.

- [`pg_exporter_url`](v-monitor.md#pg_exporter_url)
- [`pgbouncer_exporter_url`](v-monitor.md#pgbouncer_exporter_url)

As a sample, the connection string to the database for the meta-node in the sandbox environment is

```bash
PG_EXPORTER_URL='postgres://dbuser_monitor:DBUser.Monitor@:5432/postgres?host=/var/run/postgresql&sslmode=disable'
```

> ### Lazy solution
>
> If you don't care much about security and permissions, you can also use dbsu ident authentication directly, for example `postgres` users for monitoring.
>
> `pg_exporter` is executed as `dbsu` user by default, if you allow `dbsu` to access the database with local `ident` authentication free (Pigsty default configuration), you can monitor the database directly with superuser.
>
> Pigsty **highly discouraged** this deployment method, but it is really convenient in that neither new users have to be created nor permissions have to be configured.
>
> ```bash
> PG_EXPORTER_URL='postgres:///postgres?host=/var/run/postgresql&sslmode=disable'
> ```



## Related parameters

When using **monitoring deployments only**, only a subset of the Pigsty parameters will be used.

#### Infrastructure section

The infrastructure and metanodes remain consistent with the regular deployment, except that the following two parameters must force the specified configuration options.

```yml
service_registry: none # Service registration must be turned off because the target environment may not have a DCS infrastructure.
prometheus_sd_method: static # Must use static file service discovery, as the target instance may not be using service discovery with service registration
```

#### Target node section

The [identity parameters] of the target node (c-config.md# identity parameters) are still mandatory, as they define the identity of the database instance in the monitoring system.

Other than that, only the [monitoring system parameters](v-monitor) usually need to be adjusted.


```yaml

#------------------------------------------------------------------------------
# MONITOR PROVISION
#------------------------------------------------------------------------------
# - install - #
exporter_install: none                        # none|yum|binary, none by default
exporter_repo_url: ''                         # if set, repo will be added to /etc/yum.repos.d/ before yum installation

# - collect - #
exporter_metrics_path: /metrics               # default metric path for pg related exporter

# - node exporter - #
node_exporter_enabled: true                   # setup node_exporter on instance
node_exporter_port: 9100                      # default port for node exporter
node_exporter_options: '--no-collector.softnet --no-collector.nvme --collector.ntp --collector.tcpstat --collector.processes'

# - pg exporter - #
pg_exporter_config: pg_exporter.yml           # default config files for pg_exporter
pg_exporter_enabled: true                     # setup pg_exporter on instance
pg_exporter_port: 9630                        # default port for pg exporter
pg_exporter_url: ''                           # optional, if not set, generate from reference parameters
pg_exporter_auto_discovery: true              # optional, discovery available database on target instance ?
pg_exporter_exclude_database: 'template0,template1,postgres' # optional, comma separated list of database that WILL NOT be monitored when auto-discovery enabled
pg_exporter_include_database: ''             # optional, comma separated list of database that WILL BE monitored when auto-discovery enabled, empty string will disable include mode
pg_exporter_options: '--log.level=info --log.format="logger:syslog?appname=pg_exporter&local=7"'

# - pgbouncer exporter - #
pgbouncer_exporter_enabled: true              # setup pgbouncer_exporter on instance (if you don't have pgbouncer, disable it)
pgbouncer_exporter_port: 9631                 # default port for pgbouncer exporter
pgbouncer_exporter_url: ''                    # optional, if not set, generate from reference parameters
pgbouncer_exporter_options: '--log.level=info --log.format="logger:syslog?appname=pgbouncer_exporter&local=7"'

# - promtail - #                              # promtail is a beta feature which requires manual deployment
promtail_enabled: true                        # enable promtail logging collector?
promtail_clean: false                         # remove promtail status file? false by default
promtail_port: 9080                           # default listen address for promtail
promtail_status_file: /tmp/promtail-status.yml
promtail_send_url: http://10.10.10.10:3100/loki/api/v1/push  # loki url to receive logs

```



```yaml
exporter_install: binary # none|yum|binary It is recommended to use the copy binary method to install Exporter
pgbouncer_exporter_enabled: false # If the target instance does not have an associated Pgbouncer instance, then Pgbouncer monitoring needs to be disabled
pg_exporter_url: '' # URL to connect to Postgres, use this parameter if the default URL collocation rules are not used
pgbouncer_exporter_url: '' # URL to connect to Pgbouncer, use this parameter if you don't use the default URL collocation rules
pg_exporter_auto_discovery: true # optional, discovery available database on target instance ?
pg_exporter_exclude_database: 'template0,template1,postgres' # optional, comma separated list of database that WILL NOT be monitored when auto- discovery enabled
pg_exporter_include_database: '' # optional, comma separated list of database that WILL BE monitored when auto-discovery enabled, empty string will disable include mode
``''



## Execute deployment

Once the parameters are tuned, execute the following script on the target cluster ``<cluster>` to complete the monitoring deployment.

```bash
. /pgsql.yml -t monitor -l <cluster>
```

! > Forget to use `-t` to specify that the `monitor` task performs database instance initialization, make sure the command is correct before executing it

Once the monitor component is deployed, you can register it to the infrastructure with the following command.

```bash
. /pgsql.yml -t register_prometheus,register_grafana -l <cluster>
```

The ``register_prometheus`` task adds the target database instance to the list of Prometheus monitoring objects, while ``register_grafana`` registers all business databases in the cluster as data sources to Grafana.



## Limitations

Pigsty Monitoring System Works closely with the Pigsty provisioning solution, and original is always the best. Although Pigsty does not recommend splitting for use, this does work, with a few limitations.

### Missing indicators

Pigsty will integrate multiple [source](m-metric.md#number of metrics) metrics, including machine nodes, databases, Pgbouncer connection pools, and Haproxy load balancers. If these components are missing from the user's own provisioning solution, the corresponding metrics will also be missing.

Usually Node and PG monitoring metrics are always present, while the absence of PGbouncer and Haproxy usually results in a loss of metrics ranging from **100 to 200**.

In particular, the Pgbouncer monitoring metrics contain the extremely important PG QPS, TPS, and RT, which are **not available** from PostgreSQL itself.

### Service Discovery

External provisioning solutions usually have their own identity management mechanisms, so Pigsty does not overstep its authority to deploy DCS for **service discovery**. This means that users can only manage the identity of monitored objects using **static configuration files**, which is usually not a problem since Pigsty v1.0.0 uses a static file-based service discovery mechanism by default.

### Identity changes

In Pigsty sandbox, when the role identity of the instance changes, the system will correct the role information of the instance in time through the callback function and anti-entropy process, such as changing ``primary`` to ``replica`` and other roles to ``primary``.

```json
pg_up{cls="pg-meta", ins="pg-meta-1", instance="10.10.10.10:9630", ip="10.10.10.10", job="pg"}
```

Identity-related tags (e.g. `svc`, `role`) are not used in Pigsty's monitoring system, so the time series tags will not change due to master-slave switching.
If your external systems, scripts, tools use role information (`service`, `role`) from the Consul service registry, it is necessary to be concerned about identity changes due to automatic master-slave switching.

### Administrative privileges

Pigsty's monitoring metrics rely on `node_exporter` and `pg_exporter` for access.

While `pg_exporter` can be deployed in a way that the exporter pulls information about remote database instances, `node_exporter` must be deployed on the node to which the database belongs.

This means that the user must have SSH login and `sudo` privileges on the machine where the database is located in order to complete the deployment. This permission is only required at deployment time: the target node must be available for Ansible **inclusion in management**, whereas cloud vendor RDS does not usually give such permission.
